{
  "model_version": "2.0",
  "steps": [
    {
      "index": 1,
      "title": "Setup Environment",
      "blocks": [
        {
          "type": "code",
          "language": "python",
          "content": "import pandas as pd\npd.set_option('display.max_rows', 800)\npd.set_option('display.max_columns', 500)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nfrom sklearn.model_selection import GroupShuffleSplit, GroupKFold\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, brier_score_loss, log_loss, roc_auc_score, roc_curve, auc, precision_recall_curve, average_precision_score, RocCurveDisplay, PrecisionRecallDisplay, ConfusionMatrixDisplay \nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.calibration import calibration_curve\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier, log_evaluation, early_stopping\nimport re\nimport nfl_data_py as nfl\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")"
        }
      ]
    },
    {
      "index": 2,
      "title": "Load Data",
      "blocks": [
        {
          "type": "code",
          "language": "python",
          "content": "years = [2024]\nplay_by_play = nfl.import_pbp_data(years, downcast=True, cache=False)"
        },
        {
          "type": "stdout",
          "content": "2024 done.\nDowncasting floats.\n"
        }
      ]
    },
    {
      "index": 3,
      "title": "Feature Engineering",
      "blocks": [
        {
          "type": "markdown",
          "content": "<ol>\n    <li>Filter out unnecessary text, IDs, and meaningless fields</li>\n    <li>Build the next play label</li>\n    <li>Build the next posession team label. We want to make sure the row captures who has the ball next.</li>\n</ol>"
        },
        {
          "type": "code",
          "language": "python",
          "content": "# Drop columns that are IDs, free text, or meaningless at decision time\ndef prepare_next_play_dataset(\n    df: pd.DataFrame,\n    id_and_text_drop=None,\n    run_labels=(\"run\", \"rush\"),   # handle naming differences\n    pass_labels=(\"pass\",),\n    game_sort_cols=(\"game_id\", \"game_seconds_remaining\", \"play_id\")\n):\n    \"\"\"\n    Build a leakage-safe dataset for predicting the *next* play (pass/run) by the *same offense*.\n    Returns: X (features), y (0=run,1=pass), groups (game_id), base_df (cleaned working frame)\n    \"\"\"\n\n    # Sort plays within games (descending clock; play_id ensures tie-break)\n    sort_cols = list(game_sort_cols)\n    ascending = [True] + [False] * (len(sort_cols) - 1)  # game_id asc, then desc for the rest\n    df = df.sort_values(sort_cols, ascending=ascending).copy()\n\n    # Build next-play fields within each game\n    g = df.groupby(\"game_id\", sort=False)\n\n    # Who has the ball next?\n    df[\"next_posteam\"] = g[\"posteam\"].shift(-1)\n    next_playtype_col = (\n        g[\"play_type_simple\"].shift(-1)\n        if \"play_type_simple\" in df.columns\n        else g[\"play_type\"].shift(-1)\n    )\n    df[\"next_playtype\"] = next_playtype_col.astype(str).str.lower()\n\n    # Keep only rows where the same team will snap the next play\n    same_team_mask = (df[\"next_posteam\"] == df[\"posteam\"])\n\n    # Keep only rows where the next play is a pure run or pass\n    run_set  = set(s.lower() for s in run_labels)\n    pass_set = set(s.lower() for s in pass_labels)\n    next_is_run  = df[\"next_playtype\"].isin(run_set)\n    next_is_pass = df[\"next_playtype\"].isin(pass_set)\n\n    keep_mask = same_team_mask & (next_is_run | next_is_pass)\n\n    work = df.loc[keep_mask].copy()\n\n    # Create the binary label for the next play\n    work[\"target_pass_next\"] = (work[\"next_playtype\"].isin(pass_set)).astype(int)\n\n    # Drop obvious leakage/noise: IDs, free text, timestamps that uniquely ID a play, etc.\n    default_id_and_text_drop = [\n        # free text / near-unique\n        \"desc\",\n        # pure identifiers / hashes\n        \"old_game_id\", \"play_id\",\n        # raw time strings\n        \"drive_real_start_time\", \"drive_game_clock_start\", \"drive_game_clock_end\",\n        # duplicative drive boundary IDs\n        \"drive_play_id_started\", \"drive_play_id_ended\",\n    ]\n    if id_and_text_drop is None:\n        id_and_text_drop = default_id_and_text_drop\n    to_drop = [c for c in id_and_text_drop if c in work.columns]\n    if to_drop:\n        work = work.drop(columns=to_drop)\n\n    # Keep game_id for grouping, but exclude it from X later\n    groups = work[\"game_id\"].copy()\n\n    # Remove helper/label columns from features\n    helper_cols = [\"next_posteam\", \"next_playtype\", \"target_pass_next\"]\n    feature_cols = [c for c in work.columns if c not in (helper_cols + [\"game_id\"])]\n\n    # Cast object\u2192category to help LightGBM\n    obj_cols = [c for c in feature_cols if work[c].dtype == \"object\"]\n    for c in obj_cols:\n        work[c] = work[c].astype(\"category\")\n\n    X = work[feature_cols]\n    y = work[\"target_pass_next\"].astype(int)\n\n    return X, y, groups, work"
        },
        {
          "type": "markdown",
          "content": "<p>Add features that an offensive coordinator would think about when calling the next play</p>\n<ol>\n    <li>What is the next down?</li>\n    <li>How many yards between the line of scrimmage and the marker?</li>\n    <li>What part of the field am I in?</li>\n    <li>Are we trailing or winning?</li>\n    <li>What is the clock situation?</li>\n</ol>"
        },
        {
          "type": "code",
          "language": "python",
          "content": "def add_next_state_features(work: pd.DataFrame) -> pd.DataFrame:\n    g = work.groupby(\"game_id\", sort=False)\n    for col in [\"down\",\"ydstogo\",\"yardline_100\",\"score_differential\",\"wp\"]:\n        work[f\"next_{col}\"] = g[col].shift(-1)\n\n    work[\"next_down_cat\"] = work[\"next_down\"].astype(\"Int64\")\n    work[\"next_yds_bin\"] = pd.cut(work[\"next_ydstogo\"],\n                                  bins=[-1,1,3,6,10,15,99],\n                                  labels=[\"1\",\"2-3\",\"4-6\",\"7-10\",\"11-15\",\"16+\"])\n    work[\"next_field_zone\"] = pd.cut(work[\"next_yardline_100\"],\n                                     bins=[-1,20,50,80,100],\n                                     labels=[\"backed_up\",\"own_half\",\"opp_half\",\"redzone\"])\n    work[\"next_score_bucket\"] = pd.cut(work[\"next_score_differential\"],\n                                       bins=[-99,-7,7,99],\n                                       labels=[\"trailing\",\"close\",\"ahead\"])\n    work[\"clock_bucket\"] = pd.cut(work[\"game_seconds_remaining\"].fillna(-1),\n                                  bins=[-1,120,240,600,1800,3600],\n                                  labels=[\"2min\",\"2-4min\",\"4-10min\",\"10-30min\",\"early\"])\n    for c in [\"next_down_cat\",\"next_yds_bin\",\"next_field_zone\",\"next_score_bucket\",\"clock_bucket\"]:\n        work[c] = work[c].astype(\"category\")\n    return work"
        },
        {
          "type": "markdown",
          "content": "<p>A game plan designed before kickoff may not be working during the game. Does the success of previous plays in the game factor into the decision making?</p>"
        },
        {
          "type": "code",
          "language": "python",
          "content": "def add_short_term_history_features(work: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Leakage-safe short-term history features.\n    Assumes columns: game_id, posteam, play_type_simple (or play_type),\n                     success, epa, yards_gained, game_seconds_remaining,\n                     optional: drive, penalty, penalty_yards, sack, shotgun, no_huddle.\n    \"\"\"\n    work = work.copy()\n\n    # --- normalize current play type ---\n    if \"play_type_simple\" in work.columns:\n        cur_type = work[\"play_type_simple\"].astype(str).str.lower()\n    else:\n        cur_type = work.get(\"play_type\", pd.Series(index=work.index, dtype=\"object\")).astype(str).str.lower()\n\n    work[\"is_pass_now\"] = (cur_type == \"pass\").astype(int)\n    work[\"is_run_now\"]  = (cur_type.isin([\"run\", \"rush\"])).astype(int)\n\n    # group helpers\n    g_team_game = work.groupby([\"game_id\", \"posteam\"], sort=False)\n\n    # recent pass-rate windows (team within game)\n    for w in (3, 5, 10):\n        work[f\"team_pass_rate_w{w}\"] = g_team_game[\"is_pass_now\"].transform(\n            lambda s: s.shift(1).rolling(w, min_periods=1).mean()\n        )\n\n    # drive-scoped pass-rate windows (use transform to keep index flat)\n    if \"drive\" in work.columns:\n        g_tg_drive = work.groupby([\"game_id\", \"posteam\", \"drive\"], sort=False)\n        for w in (3, 5, 10):\n            work[f\"drive_pass_rate_w{w}\"] = g_tg_drive[\"is_pass_now\"].transform(\n                lambda s: s.shift(1).rolling(w, min_periods=1).mean()\n            )\n\n    # streaks entering the row (consecutive same-type BEFORE this play)\n    def _prior_streak(series: pd.Series) -> pd.Series:\n        out = np.zeros(len(series), dtype=int)\n        prev = None\n        count = 0\n        vals = series.to_numpy()\n        for i, v in enumerate(vals):\n            if prev is None or v != prev:\n                count = 0\n            else:\n                count += 1\n            out[i] = count\n            prev = v\n        return pd.Series(out, index=series.index)\n\n    work[\"pass_streak\"] = g_team_game[\"is_pass_now\"].transform(_prior_streak)\n    work[\"run_streak\"]  = g_team_game[\"is_run_now\"].transform(_prior_streak)\n\n    # last play outcomes (carryover)\n    work[\"prev_success\"] = g_team_game[\"success\"].shift(1).fillna(0).astype(float)\n\n    work[\"prev_epa\"] = g_team_game[\"epa\"].shift(1).astype(float)\n    work[\"prev_epa_bucket\"] = pd.cut(\n        work[\"prev_epa\"].fillna(0.0),\n        bins=[-10, -2.0, -0.5, 0.0, 0.5, 2.0, 10],\n        labels=[\"catastrophic\",\"bad\",\"minus\",\"neutral\",\"plus\",\"explosive\"]\n    )\n\n    work[\"prev_yards\"] = g_team_game[\"yards_gained\"].shift(1).astype(float)\n    work[\"prev_yards_bucket\"] = pd.cut(\n        work[\"prev_yards\"].fillna(0.0),\n        bins=[-10, -1, 0, 3, 7, 15, 60],\n        labels=[\"loss\",\"stuffed\",\"no_gain\",\"short\",\"chunk\",\"explosive\"]\n    )\n\n    # penalties: use explicit flag if present, else proxy from penalty_yards\n    if \"penalty\" in work.columns:\n        work[\"prev_penalty_flag\"] = g_team_game[\"penalty\"].shift(1).fillna(0).astype(int)\n    else:\n        work[\"penalty_proxy\"] = (work.get(\"penalty_yards\", pd.Series(0, index=work.index)).abs() > 0).astype(int)\n        work[\"prev_penalty_flag\"] = g_team_game[\"penalty_proxy\"].shift(1).fillna(0).astype(int)\n\n    if \"sack\" in work.columns:\n        work[\"prev_sack_flag\"] = g_team_game[\"sack\"].shift(1).fillna(0).astype(int)\n\n    # tempo / formation carryover\n    work[\"delta_secs_prev\"] = (\n        g_team_game[\"game_seconds_remaining\"].shift(1) - work[\"game_seconds_remaining\"]\n    )\n\n    work[\"tempo_bucket\"] = pd.cut(\n        work[\"delta_secs_prev\"].fillna(-1),\n        bins=[-1, 10, 20, 35, 60, 120, 10000],\n        labels=[\"hyper\",\"fast\",\"normal\",\"leisurely\",\"slow\",\"stoppage\"]\n    )\n\n    if \"shotgun\" in work.columns:\n        work[\"shotgun_prev\"] = g_team_game[\"shotgun\"].shift(1).fillna(0).astype(int)\n    if \"no_huddle\" in work.columns:\n        work[\"no_huddle_prev\"] = g_team_game[\"no_huddle\"].shift(1).fillna(0).astype(int)\n\n    # keep rates as numeric\n    for c in [\"prev_epa_bucket\", \"prev_yards_bucket\", \"tempo_bucket\"]:\n        if c in work.columns:\n            work[c] = work[c].astype(\"category\")\n\n    return work"
        },
        {
          "type": "markdown",
          "content": "<p>Andy Reid is known for being pass heavy. Kyle Shanahan is known for running the ball. Based on coaching tendencies in the past weeks of the season, let's build some features based on coaching and team tendencies.</p>"
        },
        {
          "type": "code",
          "language": "python",
          "content": "def add_tendency_features(work: pd.DataFrame, k: float = 100.0, league_p: float = None) -> pd.DataFrame:\n    \"\"\"\n    Expanding (leakage-safe) tendencies with Bayesian smoothing.\n    Creates coach/team/defense priors overall, by next down, and (coach) by next distance bin.\n    Only uses past info: cumulative sums are shifted by 1 (as-of BEFORE current row).\n    \"\"\"\n    work = work.copy()\n\n    if \"is_pass_now\" not in work.columns:\n        cur_type = work.get(\"play_type_simple\", work.get(\"play_type\", pd.Series(index=work.index, dtype=\"object\"))).astype(str).str.lower()\n        work[\"is_pass_now\"] = (cur_type == \"pass\").astype(int)\n\n    work[\"down_cat_now\"] = work[\"down\"].astype(\"Int64\")\n\n    # offense coach on this play\n    if \"off_coach\" not in work.columns:\n        if {\"home_team\",\"away_team\",\"home_coach\",\"away_coach\"}.issubset(work.columns):\n            work[\"off_coach\"] = np.where(work[\"posteam\"] == work[\"home_team\"], work[\"home_coach\"], work[\"away_coach\"])\n        else:\n            work[\"off_coach\"] = work.get(\"coach\", pd.Series(\"UNK\", index=work.index))\n\n    # current distance bins\n    dist_labels = [\"1\",\"2-3\",\"4-6\",\"7-10\",\"11-15\",\"16+\"]\n    work[\"yds_bin_now\"] = pd.cut(work[\"ydstogo\"], bins=[-1,1,3,6,10,15,99], labels=dist_labels)\n\n    # league prior\n    if league_p is None:\n        league_p = float(work[\"is_pass_now\"].mean())\n\n    def _smoothed_expanding_for_condition(group_key: pd.Series, cond_bool: pd.Series) -> pd.Series:\n        gk = pd.Series(group_key, index=work.index)\n        if str(gk.dtype) == \"category\":\n            gk = gk.astype(\"object\")\n        gk = gk.where(~gk.isna(), \"UNK\")\n\n        cb = pd.Series(cond_bool, index=work.index).fillna(False).astype(int)\n\n        inc = cb\n        pass_inc = work[\"is_pass_now\"] * inc\n\n        cum_n = inc.groupby(gk).cumsum()\n        cum_p = pass_inc.groupby(gk).cumsum()\n\n        cum_n_prev = cum_n.groupby(gk).shift(1).fillna(0.0)\n        cum_p_prev = cum_p.groupby(gk).shift(1).fillna(0.0)\n\n        num = k * league_p + cum_p_prev\n        den = k + cum_n_prev\n        return num / den\n\n    # Coach: overall tendency\n    work[\"coach_pass_rate_overall\"] = _smoothed_expanding_for_condition(\n        group_key=work[\"off_coach\"],\n        cond_bool=pd.Series(True, index=work.index)\n    )\n\n    # Coach: by NEXT down\n    for d in (1,2,3,4):\n        work[f\"coach_pass_rate_down{d}_asof\"] = _smoothed_expanding_for_condition(\n            group_key=work[\"off_coach\"],\n            cond_bool=work[\"down_cat_now\"].eq(d)\n        )\n\n    def _pick_by_next_down(row, prefix):\n        nd = row.get(\"next_down\")\n        if pd.isna(nd):\n            return np.nan\n        d = int(nd)\n        return row.get(f\"{prefix}{d}_asof\", np.nan)\n\n    work[\"coach_down_pass_rate_next\"] = work.apply(\n        lambda r: _pick_by_next_down(r, \"coach_pass_rate_down\"), axis=1\n    )\n\n    # Coach: by NEXT distance bin\n    for lab in dist_labels:\n        work[f\"coach_pass_rate_dist_{lab}_asof\"] = _smoothed_expanding_for_condition(\n            group_key=work[\"off_coach\"],\n            cond_bool=work[\"yds_bin_now\"].astype(str).eq(str(lab))\n        )\n\n    def _pick_by_next_dist(row):\n        lab = row.get(\"next_yds_bin\")\n        if pd.isna(lab):\n            return np.nan\n        return row.get(f\"coach_pass_rate_dist_{str(lab)}_asof\", np.nan)\n\n    work[\"coach_dist_pass_rate_next\"] = work.apply(_pick_by_next_dist, axis=1)\n\n    # Team: overall + by NEXT down\n    work[\"team_pass_rate_overall\"] = _smoothed_expanding_for_condition(\n        group_key=work[\"posteam\"],\n        cond_bool=pd.Series(True, index=work.index)\n    )\n    for d in (1,2,3,4):\n        work[f\"team_pass_rate_down{d}_asof\"] = _smoothed_expanding_for_condition(\n            group_key=work[\"posteam\"],\n            cond_bool=work[\"down_cat_now\"].eq(d)\n        )\n    work[\"team_down_pass_rate_next\"] = work.apply(\n        lambda r: _pick_by_next_down(r, \"team_pass_rate_down\"), axis=1\n    )\n\n    # Defense: overall + by NEXT down\n    work[\"def_pass_allowed_overall\"] = _smoothed_expanding_for_condition(\n        group_key=work[\"defteam\"],\n        cond_bool=pd.Series(True, index=work.index)\n    )\n    for d in (1,2,3,4):\n        work[f\"def_pass_allowed_down{d}_asof\"] = _smoothed_expanding_for_condition(\n            group_key=work[\"defteam\"],\n            cond_bool=work[\"down_cat_now\"].eq(d)\n        )\n    work[\"def_down_pass_allowed_next\"] = work.apply(\n        lambda r: _pick_by_next_down(r, \"def_pass_allowed_down\"), axis=1\n    )\n\n    return work"
        },
        {
          "type": "markdown",
          "content": "<p>The weather plays a huge part in decision making. When it's snowing, teams are more likely to run. When the game is being played indoors and wind is not a factor, teams tend to pass more.</p>"
        },
        {
          "type": "code",
          "language": "python",
          "content": "def add_environment_features(work: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Environment + context features known before the next snap:\n      - Roof (indoor/outdoor), surface (grass/turf), temp & wind (cleaned + bins), precip flags\n      - Offense home/away on NEXT snap\n      - NEXT snap timeouts remaining for offense (and bucket)\n      - NEXT snap two-minute situation flags (by quarter / by half if available)\n    Assumes columns may include:\n      roof, surface, temp, wind, weather, humidity,\n      game_id, qtr, game_seconds_remaining, half_seconds_remaining,\n      home_team, away_team, posteam, next_posteam,\n      home_timeouts_remaining, away_timeouts_remaining\n    Works gracefully if some columns are missing.\n    \"\"\"\n    work = work.copy()\n\n    def _norm_roof(s: pd.Series) -> pd.Series:\n        x = s.astype(str).str.lower()\n        x = x.replace({\n            \"na\": np.nan, \"nan\": np.nan, \"none\": np.nan,\n            \"retractable\": \"closed\", \"open\": \"outdoors\"\n        })\n        cat = pd.Series(np.where(x.isin([\"dome\",\"closed\"]), \"indoor\",\n                         np.where(x.isin([\"outdoor\",\"outdoors\"]), \"outdoor\", \"unknown\")),\n                        index=s.index)\n        return cat.astype(\"category\")\n\n    def _norm_surface(s: pd.Series) -> pd.Series:\n        x = s.astype(str).str.lower()\n        x = x.replace({\"artificial\": \"turf\", \"fieldturf\": \"turf\", \"astroplay\": \"turf\"})\n        cat = pd.Series(np.where(x.str.contains(\"turf|artificial|astro\", na=False), \"turf\",\n                         np.where(x.str.contains(\"grass|natural\", na=False), \"grass\", \"unknown\")),\n                        index=s.index)\n        return cat.astype(\"category\")\n\n    if \"roof\" in work.columns:\n        work[\"roof_norm\"] = _norm_roof(work[\"roof\"])\n        work[\"is_indoor\"] = (work[\"roof_norm\"] == \"indoor\").astype(int)\n\n    if \"surface\" in work.columns:\n        work[\"surface_norm\"] = _norm_surface(work[\"surface\"])\n\n    # Clean temp / wind / humidity & make bins\n    def _to_float(series: pd.Series) -> pd.Series:\n        s = series.copy()\n        if pd.api.types.is_numeric_dtype(s):\n            return pd.to_numeric(s, errors=\"coerce\")\n        # extract first float in the string\n        num = s.astype(str).str.extract(r\"([-+]?\\d*\\.?\\d+)\")[0]\n        return pd.to_numeric(num, errors=\"coerce\")\n\n    if \"temp\" in work.columns:\n        work[\"temp_f\"] = _to_float(work[\"temp\"])\n        work[\"temp_bin\"] = pd.cut(\n            work[\"temp_f\"],\n            bins=[-50, 32, 50, 70, 85, 150],\n            labels=[\"freezing\",\"cool\",\"mild\",\"warm\",\"hot\"]\n        ).astype(\"category\")\n\n    if \"wind\" in work.columns:\n        work[\"wind_mph\"] = _to_float(work[\"wind\"])\n        work[\"wind_bin\"] = pd.cut(\n            work[\"wind_mph\"],\n            bins=[-1, 5, 15, 50],\n            labels=[\"calm\",\"breezy\",\"windy\"]\n        ).astype(\"category\")\n        work[\"is_windy\"] = (work[\"wind_mph\"] >= 15).astype(int)\n\n    if \"humidity\" in work.columns:\n        work[\"humidity_pct\"] = _to_float(work[\"humidity\"])\n        work[\"humidity_bin\"] = pd.cut(\n            work[\"humidity_pct\"], bins=[-1, 40, 60, 100], labels=[\"dry\",\"normal\",\"humid\"]\n        ).astype(\"category\")\n\n    # precipitation flags from free-text weather, if present\n    if \"weather\" in work.columns:\n        wtxt = work[\"weather\"].astype(str).str.lower()\n        work[\"precip_flag\"] = wtxt.str.contains(\n            r\"rain|drizzle|shower|storm|thunder|hail|sleet\", regex=True, na=False\n        ).astype(int)\n        work[\"snow_flag\"] = wtxt.str.contains(r\"snow|flurr\", regex=True, na=False).astype(int)\n        work[\"fog_flag\"]  = wtxt.str.contains(r\"fog\", regex=True, na=False).astype(int)\n\n    # If indoor, wind doesn't affect ball flight; keep both raw & interaction if you like\n    if \"is_indoor\" in work.columns and \"is_windy\" in work.columns:\n        work[\"windy_outdoor\"] = ((work[\"is_indoor\"] == 0) & (work[\"is_windy\"] == 1)).astype(int)\n\n    # NEXT-snap timeouts & home/away for offense\n    g = work.groupby(\"game_id\", sort=False)\n\n    # next_qtr / next_game_seconds for two-minute logic\n    if \"qtr\" in work.columns:\n        work[\"next_qtr\"] = g[\"qtr\"].shift(-1)\n    if \"game_seconds_remaining\" in work.columns:\n        work[\"next_game_seconds_remaining\"] = g[\"game_seconds_remaining\"].shift(-1)\n    if \"half_seconds_remaining\" in work.columns:\n        work[\"next_half_seconds_remaining\"] = g[\"half_seconds_remaining\"].shift(-1)\n\n    # Who is on offense next?\n    if \"next_posteam\" not in work.columns:\n        work[\"next_posteam\"] = g[\"posteam\"].shift(-1)\n\n    # Is that offense the home team on next snap?\n    if {\"home_team\",\"away_team\"}.issubset(work.columns):\n        work[\"is_home_offense_next\"] = np.where(\n            work[\"next_posteam\"] == work[\"home_team\"], 1,\n            np.where(work[\"next_posteam\"] == work[\"away_team\"], 0, np.nan))\n\n    # Next-snap timeouts for the offense\n    if {\"home_timeouts_remaining\",\"away_timeouts_remaining\",\"home_team\",\"away_team\"}.issubset(work.columns):\n        next_home_to = g[\"home_timeouts_remaining\"].shift(-1)\n        next_away_to = g[\"away_timeouts_remaining\"].shift(-1)\n        work[\"next_off_timeouts\"] = np.where(\n            work[\"next_posteam\"] == work[\"home_team\"], next_home_to,\n            np.where(work[\"next_posteam\"] == work[\"away_team\"], next_away_to, np.nan)\n        )\n        work[\"next_off_timeouts_bucket\"] = pd.Categorical(\n            pd.cut(work[\"next_off_timeouts\"], bins=[-1,0,1,2,3], labels=[\"0\",\"1\",\"2\",\"3\"])\n        )\n\n    # Two-minute situation on NEXT snap by quarter: final 2:00 of Q2 and Q4\n    if {\"next_qtr\",\"next_game_seconds_remaining\"}.issubset(work.columns):\n        # seconds remaining in quarter isn\u2019t directly present; approximate via mod 900 if needed.\n        # When only game_seconds_remaining exists, a simple and still useful flag is:\n        # - Q2: GSR in (1800..3600] \u2192 two-minute when GSR % 1800 <= 120\n        # - Q4: GSR in (0..1800]    \u2192 two-minute when GSR % 1800 <= 120\n        gsr = work[\"next_game_seconds_remaining\"]\n        nq = work[\"next_qtr\"]\n        # modulo within half (each half is 1800s)\n        within_half = (gsr % 1800).astype(float)\n        work[\"two_minute_next_qtr\"] = (\n            (nq.isin([2,4])) & (within_half <= 120)\n        ).astype(int)\n\n    # by half if we have it precisely\n    if \"next_half_seconds_remaining\" in work.columns:\n        work[\"two_minute_next_half\"] = (work[\"next_half_seconds_remaining\"] <= 120).astype(int)\n\n    # Cast buckets to category\n    for c in [\"temp_bin\",\"wind_bin\",\"humidity_bin\",\"next_off_timeouts_bucket\"]:\n        if c in work.columns:\n            work[c] = work[c].astype(\"category\")\n\n    # Keep raw numerics for models that benefit from continuous inputs\n    return work"
        },
        {
          "type": "code",
          "language": "python",
          "content": "def select_features_for_model(\n    work: pd.DataFrame,\n    target_col: str = \"target_pass_next\",\n    drop_raw_team_ids: bool = True,\n    missing_frac_thresh: float = 0.98,   # drop if >= 98% missing\n    high_card_thresh: int = 200,         # drop object/cat with >200 unique values\n    low_var_thresh: float = 1e-9         # drop numeric with ~zero variance\n):\n    \"\"\"\n    Prune unhelpful columns and return X, y, feature_cols, and a summary dict.\n    Assumes `work` already contains Steps 0-4 features.\n    \"\"\"\n    df = work.copy()\n\n    # 1) Always-drop helpers/labels/IDs/free text\n    always_drop = set([\n        target_col, \"game_id\", \"next_playtype\", \"next_posteam\",  # helpers/label bits\n        \"old_game_id\", \"play_id\", \"desc\",                        # IDs/text\n        \"drive_real_start_time\",\"drive_game_clock_start\",\"drive_game_clock_end\",\n        \"drive_play_id_started\",\"drive_play_id_ended\",\n        \"game_date\",\"game_clock\",\"play_clock\"  # if present as strings\n    ]) & set(df.columns)\n\n    # 2) Optional: drop raw team/coach IDs (we have smoothed tendencies already)\n    raw_ids = set([\n        \"posteam\",\"defteam\",\"home_team\",\"away_team\",\n        \"home_coach\",\"away_coach\",\"off_coach\"\n    ]) & set(df.columns)\n    if drop_raw_team_ids:\n        always_drop |= raw_ids\n\n    # 3) Leakage guard: drop any accidental \"next_*\" that isn\u2019t on our whitelist\n    next_whitelist = {\n        \"next_down\",\"next_ydstogo\",\"next_yardline_100\",\"next_score_differential\",\"next_wp\",\n        \"next_down_cat\",\"next_yds_bin\",\"next_field_zone\",\"next_score_bucket\",\n        \"next_qtr\",\"next_game_seconds_remaining\",\"next_half_seconds_remaining\",\n        \"next_off_timeouts\",\"next_off_timeouts_bucket\"\n    }\n    next_cols = {c for c in df.columns if c.startswith(\"next_\")}\n    leak_next = next_cols - next_whitelist\n    always_drop |= leak_next\n\n    # 4) High-missing columns\n    missing_frac = df.isna().mean()\n    drop_missing = set(missing_frac[missing_frac >= missing_frac_thresh].index)\n\n    # 5) Low-variance numeric columns\n    num_cols = df.select_dtypes(include=[np.number]).columns\n    near_const = set()\n    for c in num_cols:\n        s = df[c]\n        # treat NaNs as ignorable; check variance on non-nans\n        vals = s.dropna()\n        if vals.empty or vals.nunique() <= 1 or float(vals.var()) <= low_var_thresh:\n            near_const.add(c)\n\n    # 6) High-cardinality object/categorical columns (whitelist engineered buckets)\n    whitelist_smallcats = {\n        \"next_down_cat\",\"next_yds_bin\",\"next_field_zone\",\"next_score_bucket\",\"clock_bucket\",\n        \"prev_epa_bucket\",\"prev_yards_bucket\",\"tempo_bucket\",\n        \"temp_bin\",\"wind_bin\",\"humidity_bin\",\n        \"roof_norm\",\"surface_norm\",\"is_indoor\",\"is_windy\",\"windy_outdoor\",\n        \"is_home_offense_next\",\"next_off_timeouts_bucket\"\n    }\n    obj_cat_cols = df.select_dtypes(include=[\"object\",\"category\"]).columns\n    high_card = set()\n    for c in obj_cat_cols:\n        if c in whitelist_smallcats:\n            continue\n        nunq = df[c].nunique(dropna=True)\n        if nunq > high_card_thresh:\n            high_card.add(c)\n\n    # 7) Build final drop set\n    drop_cols = (always_drop | drop_missing | near_const | high_card) & set(df.columns)\n\n    # 8) Final feature list\n    feature_cols = [c for c in df.columns if c not in drop_cols]\n\n    # LightGBM likes categories; ensure category dtype for our bucket cols\n    cat_cols = df[feature_cols].select_dtypes(include=\"object\").columns\n    for c in cat_cols:\n        df[c] = df[c].astype(\"category\")\n\n    X = df[feature_cols]\n    y = df[target_col].astype(int)\n\n    summary = {\n        \"dropped_total\": len(drop_cols),\n        \"dropped_helpers\": sorted(always_drop & set(df.columns)),\n        \"dropped_missing\": sorted(drop_missing),\n        \"dropped_low_variance\": sorted(near_const),\n        \"dropped_high_cardinality\": sorted(high_card),\n        \"n_features_final\": len(feature_cols)\n    }\n    return X, y, feature_cols, summary"
        },
        {
          "type": "code",
          "language": "python",
          "content": "# Step 1\nX0, y0, groups0, work = prepare_next_play_dataset(play_by_play)   # df = your play-by-play DataFrame\n\n# Step 2\nwork = add_next_state_features(work)\n\n# Step 3\nwork = add_short_term_history_features(work)\n\n# Step 4\nwork = add_tendency_features(work, k=100.0)\n\n# Step 5\nwork = add_environment_features(work)\n\n# Step 6\nX_all, y_all, feature_cols, drop_summary = select_features_for_model(\n    work,\n    drop_raw_team_ids=True,\n    missing_frac_thresh=0.98,\n    high_card_thresh=200,\n    low_var_thresh=1e-9\n)\n\nprint(\"Dropped\", drop_summary[\"dropped_total\"], \"columns. Final feature count:\", drop_summary[\"n_features_final\"])"
        },
        {
          "type": "stdout",
          "content": "Dropped 149 columns. Final feature count: 297\n"
        }
      ]
    },
    {
      "index": 4,
      "title": "Model Building",
      "blocks": [
        {
          "type": "code",
          "language": "python",
          "content": "def group_train_val_test_split(groups: pd.Series, test_size=0.2, val_size=0.2, random_state=42):\n    \"\"\"\n    Group (by game_id) split into train/val/test without leakage.\n    Returns: (train_idx, val_idx, test_idx)\n    \"\"\"\n    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n    # First split off test\n    all_idx = np.arange(len(groups))\n    trainval_idx, test_idx = next(gss.split(all_idx, groups=groups))\n\n    # Now split train/val within the remaining pool\n    gss_inner = GroupShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state)\n    train_idx, val_idx = next(gss_inner.split(trainval_idx, groups=groups.iloc[trainval_idx]))\n\n    # Map inner indices back to absolute indices\n    train_idx = trainval_idx[train_idx]\n    val_idx   = trainval_idx[val_idx]\n\n    return train_idx, val_idx, test_idx"
        },
        {
          "type": "code",
          "language": "python",
          "content": "groups = work.loc[X_all.index, \"game_id\"]\ntrain_idx, val_idx, test_idx = group_train_val_test_split(groups, test_size=0.2, val_size=0.2, random_state=42)\n\nX_train_raw, y_train = X_all.iloc[train_idx].copy(), y_all.iloc[train_idx].copy()\nX_val_raw,   y_val   = X_all.iloc[val_idx].copy(),   y_all.iloc[val_idx].copy()\nX_test_raw,  y_test  = X_all.iloc[test_idx].copy(),  y_all.iloc[test_idx].copy()\n\nfor df_split in (X_train_raw, X_val_raw, X_test_raw):\n    obj_cols = df_split.select_dtypes(include=[\"object\"]).columns\n    for c in obj_cols:\n        df_split[c] = df_split[c].astype(\"category\")\n\ncategorical_cols = list(X_train_raw.select_dtypes(include=[\"category\"]).columns)\n\nfor c in categorical_cols:\n    cats = (\n        X_train_raw[c].astype(\"category\").cat.categories\n        .union(X_val_raw[c].astype(\"category\").cat.categories)\n        .union(X_test_raw[c].astype(\"category\").cat.categories)\n    )\n    X_train_raw[c] = X_train_raw[c].astype(\"category\").cat.set_categories(cats)\n    X_val_raw[c]   = X_val_raw[c].astype(\"category\").cat.set_categories(cats)\n    X_test_raw[c]  = X_test_raw[c].astype(\"category\").cat.set_categories(cats)\n\nclf = LGBMClassifier(\n    n_estimators=2000,\n    learning_rate=0.05,\n    num_leaves=31,\n    random_state=42,\n    n_jobs=-1\n)\n\nclf.fit(\n    X_train_raw, y_train,\n    eval_set=[(X_val_raw, y_val)],\n    eval_metric=\"logloss\",\n    categorical_feature=categorical_cols,\n    callbacks=[early_stopping(100), log_evaluation(50)]\n)\n\nbest_iter = clf.best_iteration_\nprint(\"Best iteration:\", best_iter)\n\ndef evaluate_split(name, X, y, model, thr=0.5):\n    proba = model.predict_proba(X, num_iteration=best_iter)[:, 1]\n    pred  = (proba >= thr).astype(int)\n    acc   = accuracy_score(y, pred)\n    ll    = log_loss(y, proba, labels=[0,1])\n    auc   = roc_auc_score(y, proba)\n    brier = brier_score_loss(y, proba)\n    cm    = confusion_matrix(y, pred, labels=[0,1])\n    print(f\"\\n=== {name} ===\")\n    print(f\"Accuracy: {acc:.4f} | LogLoss: {ll:.4f} | AUC: {auc:.4f} | Brier: {brier:.4f}\")\n    print(\"Confusion matrix [[TN FP],[FN TP]]:\\n\", cm)\n    return proba, pred\n\n_ = evaluate_split(\"Validation\", X_val_raw,  y_val,  clf)\n_ = evaluate_split(\"Test\",        X_test_raw, y_test, clf)\n\nprint(\"\\nClassification report (Test):\")\nprint(classification_report(y_test, (clf.predict_proba(X_test_raw, num_iteration=best_iter)[:,1] >= 0.5).astype(int)))\n\n# Feature importance (top 30)\nimport pandas as pd\ngain_imp  = pd.Series(clf.booster_.feature_importance(importance_type=\"gain\"),  index=X_train_raw.columns).sort_values(ascending=False)\nsplit_imp = pd.Series(clf.booster_.feature_importance(importance_type=\"split\"), index=X_train_raw.columns).sort_values(ascending=False)\n\nprint(\"\\nTop features by GAIN:\\n\",  gain_imp.head(30))\nprint(\"\\nTop features by SPLIT:\\n\", split_imp.head(30))"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Info] Number of positive: 10045, number of negative: 7118\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011213 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 27879\n[LightGBM] [Info] Number of data points in the train set: 17163, number of used features: 294\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.585271 -> initscore=0.344448\n[LightGBM] [Info] Start training from score 0.344448\nTraining until validation scores don't improve for 100 rounds\n"
        },
        {
          "type": "stdout",
          "content": "[50]\tvalid_0's binary_logloss: 0.57353\n"
        },
        {
          "type": "stdout",
          "content": "[100]\tvalid_0's binary_logloss: 0.566338\n[150]\tvalid_0's binary_logloss: 0.565212\n"
        },
        {
          "type": "stdout",
          "content": "[200]\tvalid_0's binary_logloss: 0.565352\n[250]\tvalid_0's binary_logloss: 0.567273\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[166]\tvalid_0's binary_logloss: 0.565107\nBest iteration: 166\n\n=== Validation ===\nAccuracy: 0.7085 | LogLoss: 0.5651 | AUC: 0.7677 | Brier: 0.1915\nConfusion matrix [[TN FP],[FN TP]]:\n [[1123  664]\n [ 596 1939]]\n\n=== Test ===\nAccuracy: 0.7006 | LogLoss: 0.5659 | AUC: 0.7635 | Brier: 0.1923\nConfusion matrix [[TN FP],[FN TP]]:\n [[1335  853]\n [ 768 2458]]\n\nClassification report (Test):\n              precision    recall  f1-score   support\n\n           0       0.63      0.61      0.62      2188\n           1       0.74      0.76      0.75      3226\n\n    accuracy                           0.70      5414\n   macro avg       0.69      0.69      0.69      5414\nweighted avg       0.70      0.70      0.70      5414\n\n\nTop features by GAIN:\n epa                            13974.743148\nxpass                           7272.291310\nside_of_field                   4948.932336\nstadium                         3615.663774\nnext_ydstogo                    3019.286475\npasser                          2557.734698\nhalf_seconds_remaining          2370.182030\nsolo_tackle_1_team              2275.788968\ndef_wp                          2271.823564\npass_oe                         1827.045866\nwp                              1690.173710\npasser_player_id                1610.873110\nseries_result                   1606.901433\nnext_wp                         1374.960288\ncoach_dist_pass_rate_next       1352.785792\nnext_half_seconds_remaining     1144.079252\nnext_yardline_100               1076.099418\nnext_down                       1060.254475\ndown                             924.274721\nvegas_wpa                        908.457454\ndef_down_pass_allowed_next       871.411023\ntd_prob                          791.650863\ndrive_end_transition             777.469310\nno_score_prob                    736.446972\ncoach_down_pass_rate_next        723.780717\nassist_tackle_1_team             654.171864\nrushing_yards                    643.173314\nep                               597.964406\nyards_gained                     567.136584\nprev_epa                         534.932029\ndtype: float64\n\nTop features by SPLIT:\n side_of_field                  348\nstadium                        249\nsolo_tackle_1_team             157\nxpass                          149\npasser                         145\nepa                            131\nnext_ydstogo                   124\npass_oe                        109\npasser_player_id                93\nnext_wp                         79\nnext_yardline_100               77\ncoach_dist_pass_rate_next       77\ndef_down_pass_allowed_next      76\ncoach_down_pass_rate_next       67\nnext_half_seconds_remaining     65\nvegas_wpa                       64\ntd_prob                         63\ndrive_end_transition            55\nwp                              54\nvegas_home_wpa                  53\nassist_tackle_1_team            51\ndef_wp                          50\nprev_epa                        50\ndelta_secs_prev                 48\nhalf_seconds_remaining          48\nep                              46\nquarter_seconds_remaining       44\nyards_gained                    44\ndown                            43\nydsnet                          43\ndtype: int32\n"
        }
      ]
    },
    {
      "index": 5,
      "title": "Fix Overfitting",
      "blocks": [
        {
          "type": "markdown",
          "content": "<p>We might be overfitting. The accuracy is around 70%, but we can improve that.</p>"
        },
        {
          "type": "code",
          "language": "python",
          "content": "# Recompute groups aligned to X_all (from select_features_for_model)\ngroups = work.loc[X_all.index, \"game_id\"]\n\n# cast object->category and align categories across splits\ndef prepare_cats(Xtr, Xva, Xte):\n    for df_split in (Xtr, Xva, Xte):\n        obj_cols = df_split.select_dtypes(include=[\"object\"]).columns\n        for c in obj_cols:\n            df_split[c] = df_split[c].astype(\"category\")\n    categorical_cols = list(Xtr.select_dtypes(include=[\"category\"]).columns)\n    for c in categorical_cols:\n        cats = (\n            Xtr[c].cat.categories\n            .union(Xva[c].cat.categories)\n            .union(Xte[c].cat.categories)\n        )\n        Xtr[c] = Xtr[c].cat.set_categories(cats)\n        Xva[c] = Xva[c].cat.set_categories(cats)\n        Xte[c] = Xte[c].cat.set_categories(cats)\n    return categorical_cols\n\n# find best threshold on validation for accuracy\ndef best_threshold(y_true, proba):\n    thr_grid = np.linspace(0.3, 0.7, 81)\n    accs = [(thr, accuracy_score(y_true, (proba >= thr).astype(int))) for thr in thr_grid]\n    thr, acc = max(accs, key=lambda t: t[1])\n    return thr, acc\n\ndef train_eval_variant(name, keep_cols):\n    train_idx, val_idx, test_idx = group_train_val_test_split(groups, test_size=0.2, val_size=0.2, random_state=42)\n\n    Xtr_raw, ytr = X_all.loc[X_all.index[train_idx], keep_cols].copy(), y_all.iloc[train_idx].copy()\n    Xva_raw, yva = X_all.loc[X_all.index[val_idx],   keep_cols].copy(), y_all.iloc[val_idx].copy()\n    Xte_raw, yte = X_all.loc[X_all.index[test_idx],  keep_cols].copy(), y_all.iloc[test_idx].copy()\n\n    categorical_cols = prepare_cats(Xtr_raw, Xva_raw, Xte_raw)\n\n    clf = LGBMClassifier(\n        n_estimators=2000,\n        learning_rate=0.05,\n        num_leaves=31,\n        random_state=42,\n        n_jobs=-1\n    )\n\n    clf.fit(\n        Xtr_raw, ytr,\n        eval_set=[(Xva_raw, yva)],\n        eval_metric=\"logloss\",\n        categorical_feature=categorical_cols,\n        callbacks=[early_stopping(100), log_evaluation(50)]\n    )\n\n    best_iter = clf.best_iteration_\n    proba_val = clf.predict_proba(Xva_raw, num_iteration=best_iter)[:,1]\n    thr, _ = best_threshold(yva, proba_val)\n\n    def report(split, X, y):\n        p = clf.predict_proba(X, num_iteration=best_iter)[:,1]\n        preds = (p >= thr).astype(int)\n        metrics = {\n            \"thr\": thr,\n            \"acc\": accuracy_score(y, preds),\n            \"logloss\": log_loss(y, p, labels=[0,1]),\n            \"auc\": roc_auc_score(y, p),\n            \"brier\": brier_score_loss(y, p),\n        }\n        return metrics\n\n    val_metrics  = report(\"val\",  Xva_raw, yva)\n    test_metrics = report(\"test\", Xte_raw, yte)\n\n    print(f\"\\n=== {name} ===\")\n    print(f\"Val:  acc {val_metrics['acc']:.4f} | logloss {val_metrics['logloss']:.4f} | AUC {val_metrics['auc']:.4f} | Brier {val_metrics['brier']:.4f} | thr {val_metrics['thr']:.3f}\")\n    print(f\"Test: acc {test_metrics['acc']:.4f} | logloss {test_metrics['logloss']:.4f} | AUC {test_metrics['auc']:.4f} | Brier {test_metrics['brier']:.4f} | thr {val_metrics['thr']:.3f}\")\n\n    return {\n        \"model\": clf, \"best_iter\": best_iter, \"thr\": thr,\n        \"val\": val_metrics, \"test\": test_metrics,\n        \"keep_cols\": keep_cols\n    }\n\nall_cols = list(X_all.columns)\n\n# regex for player/crew identifiers we want to drop\nplayer_regex = re.compile(\n    r\"(passer|rusher|receiver|tackle|assist_tackle|qb_hit|sack_player|\"\n    r\"pass_defense|intercept|lateral|fumble|penalty_player|player_id|player_name)\",\n    flags=re.I\n)\n\n# baseline: keep everything you selected in Step 5\nvariants = {\"baseline\": all_cols}\n\n# drop player/crew identity columns + stadium (overfit/venue proxy)\nno_players_stadium = [c for c in all_cols if (not player_regex.search(c)) and c not in {\"stadium\"}]\nvariants[\"no_players_stadium\"] = no_players_stadium\n\n# also drop current-play priors (xpass / pass_oe)\nno_players_stadium_no_xpass = [c for c in no_players_stadium if c not in {\"xpass\",\"pass_oe\"}]\nvariants[\"no_players_stadium_no_xpass\"] = no_players_stadium_no_xpass\n\nresults = {}\nfor name, cols in variants.items():\n    results[name] = train_eval_variant(name, cols)\n\n# inspect which variant wins on validation accuracy/AUC etc.\nbest = max(results.items(), key=lambda kv: kv[1][\"val\"][\"auc\"])\nprint(\"\\nBest by Val AUC:\", best[0], best[1][\"val\"])"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Info] Number of positive: 10045, number of negative: 7118\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011523 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 27879\n[LightGBM] [Info] Number of data points in the train set: 17163, number of used features: 294\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.585271 -> initscore=0.344448\n[LightGBM] [Info] Start training from score 0.344448\nTraining until validation scores don't improve for 100 rounds\n"
        },
        {
          "type": "stdout",
          "content": "[50]\tvalid_0's binary_logloss: 0.57353\n"
        },
        {
          "type": "stdout",
          "content": "[100]\tvalid_0's binary_logloss: 0.566338\n[150]\tvalid_0's binary_logloss: 0.565212\n"
        },
        {
          "type": "stdout",
          "content": "[200]\tvalid_0's binary_logloss: 0.565352\n[250]\tvalid_0's binary_logloss: 0.567273\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[166]\tvalid_0's binary_logloss: 0.565107\n\n=== baseline ===\nVal:  acc 0.7103 | logloss 0.5651 | AUC 0.7677 | Brier 0.1915 | thr 0.480\nTest: acc 0.6971 | logloss 0.5659 | AUC 0.7635 | Brier 0.1923 | thr 0.480\n"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Info] Number of positive: 10045, number of negative: 7118\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011369 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 27187\n[LightGBM] [Info] Number of data points in the train set: 17163, number of used features: 266\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.585271 -> initscore=0.344448\n[LightGBM] [Info] Start training from score 0.344448\nTraining until validation scores don't improve for 100 rounds\n"
        },
        {
          "type": "stdout",
          "content": "[50]\tvalid_0's binary_logloss: 0.571169\n[100]\tvalid_0's binary_logloss: 0.565128\n"
        },
        {
          "type": "stdout",
          "content": "[150]\tvalid_0's binary_logloss: 0.563892\n[200]\tvalid_0's binary_logloss: 0.565176\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[144]\tvalid_0's binary_logloss: 0.563624\n\n=== no_players_stadium ===\nVal:  acc 0.7122 | logloss 0.5636 | AUC 0.7702 | Brier 0.1908 | thr 0.485\nTest: acc 0.7041 | logloss 0.5658 | AUC 0.7639 | Brier 0.1922 | thr 0.485\n"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Info] Number of positive: 10045, number of negative: 7118\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010316 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26677\n[LightGBM] [Info] Number of data points in the train set: 17163, number of used features: 264\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.585271 -> initscore=0.344448\n[LightGBM] [Info] Start training from score 0.344448\nTraining until validation scores don't improve for 100 rounds\n"
        },
        {
          "type": "stdout",
          "content": "[50]\tvalid_0's binary_logloss: 0.574042\n[100]\tvalid_0's binary_logloss: 0.567313\n"
        },
        {
          "type": "stdout",
          "content": "[150]\tvalid_0's binary_logloss: 0.566393\n[200]\tvalid_0's binary_logloss: 0.566944\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[149]\tvalid_0's binary_logloss: 0.56635\n\n=== no_players_stadium_no_xpass ===\nVal:  acc 0.7008 | logloss 0.5663 | AUC 0.7662 | Brier 0.1922 | thr 0.445\nTest: acc 0.6967 | logloss 0.5679 | AUC 0.7621 | Brier 0.1928 | thr 0.445\n\nBest by Val AUC: no_players_stadium {'thr': 0.485, 'acc': 0.7121702915316983, 'logloss': 0.5636243726377981, 'auc': 0.7701994130301134, 'brier': 0.19080175024358725}\n"
        },
        {
          "type": "markdown",
          "content": "<p>Strong lift. Dropping player IDs + stadium gave  us the best Val AUC (0.770) and a nice Test acc bump (0.704), and keeping xpass/pass_oe helps. Lock that variant and then 1. tune a bit, 2. calibrate, 3. sanity-check by situation.</p>"
        },
        {
          "type": "code",
          "language": "python",
          "content": "# Use the no_players_stadium keep list you already built:\nkeep_cols = no_players_stadium  # from your ablation step\n\n# Rebuild splits (same seed to keep comparability)\ngroups = work.loc[X_all.index, \"game_id\"]\ntrain_idx, val_idx, test_idx = group_train_val_test_split(groups, test_size=0.2, val_size=0.2, random_state=42)\n\n# Merge train+val for the final model (still use early stopping on a tiny val slice)\ntrval_idx = np.concatenate([train_idx, val_idx])\nX_trval_raw = X_all.loc[X_all.index[trval_idx], keep_cols].copy()\ny_trval     = y_all.iloc[trval_idx].copy()\n\nX_test_raw  = X_all.loc[X_all.index[test_idx],  keep_cols].copy()\ny_test      = y_all.iloc[test_idx].copy()\n\n# Categories (align levels across splits)\nfor df_split in (X_trval_raw, X_test_raw):\n    for c in df_split.select_dtypes(include=\"object\").columns:\n        df_split[c] = df_split[c].astype(\"category\")\n\ncategorical_cols = list(X_trval_raw.select_dtypes(include=\"category\").columns)\nfor c in categorical_cols:\n    cats = X_trval_raw[c].cat.categories.union(X_test_raw[c].cat.categories)\n    X_trval_raw[c] = X_trval_raw[c].cat.set_categories(cats)\n    X_test_raw[c]  = X_test_raw[c].cat.set_categories(cats)\n\n# Tiny early-stopping val (group-aware)\n# Build groups for the combined train+val pool\ngroups_trval = groups.iloc[trval_idx].reset_index(drop=True)\n\n# Reindex features/labels to match\nX_trval_raw = X_trval_raw.reset_index(drop=True)\ny_trval     = y_trval.reset_index(drop=True)\n\n# 10% of train+val as early-stopping validation (grouped by game_id)\ngss = GroupShuffleSplit(n_splits=1, test_size=0.10, random_state=123)\nsub_train_rel, sub_val_rel = next(gss.split(np.arange(len(groups_trval)), groups=groups_trval))\n\nX_subtr, y_subtr = X_trval_raw.iloc[sub_train_rel], y_trval.iloc[sub_train_rel]\nX_subva, y_subva = X_trval_raw.iloc[sub_val_rel],  y_trval.iloc[sub_val_rel]\n\nfrom lightgbm import LGBMClassifier, early_stopping, log_evaluation\n\nfinal_clf = LGBMClassifier(\n    n_estimators=4000,\n    learning_rate=0.035,\n    num_leaves=63,\n    min_child_samples=40,\n    feature_fraction=0.85,\n    bagging_fraction=0.8,\n    bagging_freq=1,\n    lambda_l2=0.0,\n    random_state=42,\n    n_jobs=-1\n)\n\nfinal_clf.fit(\n    X_subtr, y_subtr,\n    eval_set=[(X_subva, y_subva)],\n    eval_metric=\"logloss\",\n    categorical_feature=categorical_cols,\n    callbacks=[early_stopping(150), log_evaluation(100)]\n)\n\nbest_iter = final_clf.best_iteration_\nprint(\"Final best_iter:\", best_iter)\n\n# Evaluate on held-out TEST\nfrom sklearn.metrics import accuracy_score, log_loss, roc_auc_score, brier_score_loss, confusion_matrix\nproba_test = final_clf.predict_proba(X_test_raw, num_iteration=best_iter)[:,1]\n\n# Use the val-optimized threshold you found (~0.485) or re-tune on X_subva/y_subva\nthr = 0.485\npred_test = (proba_test >= thr).astype(int)\nprint(\"Test ACC:\", accuracy_score(y_test, pred_test),\n      \"AUC:\", roc_auc_score(y_test, proba_test),\n      \"LogLoss:\", log_loss(y_test, proba_test),\n      \"Brier:\", brier_score_loss(y_test, proba_test))\nprint(\"CM:\\n\", confusion_matrix(y_test, pred_test))"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] Number of positive: 11350, number of negative: 7955\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013806 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 27224\n[LightGBM] [Info] Number of data points in the train set: 19305, number of used features: 263\n[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.587931 -> initscore=0.355417\n[LightGBM] [Info] Start training from score 0.355417\nTraining until validation scores don't improve for 150 rounds\n"
        },
        {
          "type": "stdout",
          "content": "[100]\tvalid_0's binary_logloss: 0.558273\n"
        },
        {
          "type": "stdout",
          "content": "[200]\tvalid_0's binary_logloss: 0.556751\n"
        },
        {
          "type": "stdout",
          "content": "[300]\tvalid_0's binary_logloss: 0.558071\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[234]\tvalid_0's binary_logloss: 0.556067\nFinal best_iter: 234\n[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nTest ACC: 0.699113409678611 AUC: 0.7653442210286396 LogLoss: 0.5642674989035821 Brier: 0.1916863498775012\nCM:\n [[1215  973]\n [ 656 2570]]\n"
        },
        {
          "type": "code",
          "language": "python",
          "content": "rng = np.random.RandomState(7)\n\ndef cv_score(params, X, y, groups, cat_cols, n_splits=4):\n    gkf = GroupKFold(n_splits=n_splits)\n    scores = []\n    for tr_idx, va_idx in gkf.split(X, y, groups):\n        Xtr, Xva = X.iloc[tr_idx].copy(), X.iloc[va_idx].copy()\n        ytr, yva = y.iloc[tr_idx], y.iloc[va_idx]\n        # ensure cats across split\n        for df_split in (Xtr, Xva):\n            for c in Xtr.select_dtypes(include=\"object\").columns:\n                df_split[c] = df_split[c].astype(\"category\")\n        cats = list(Xtr.select_dtypes(include=\"category\").columns)\n        for c in cats:\n            union = Xtr[c].cat.categories.union(Xva[c].cat.categories)\n            Xtr[c] = Xtr[c].cat.set_categories(union)\n            Xva[c] = Xva[c].cat.set_categories(union)\n\n        clf = LGBMClassifier(n_estimators=3000, random_state=42, n_jobs=-1, **params)\n        clf.fit(\n            Xtr, ytr,\n            eval_set=[(Xva, yva)],\n            eval_metric=\"logloss\",\n            categorical_feature=cats,\n            callbacks=[early_stopping(150)]\n        )\n        p = clf.predict_proba(Xva, num_iteration=clf.best_iteration_)[:,1]\n        scores.append(log_loss(yva, p))\n    return np.mean(scores)\n\nsearch_space = {\n    \"learning_rate\": lambda: 10**rng.uniform(-2.0, -1.2),  # ~0.01\u20130.06\n    \"num_leaves\":    lambda: int(rng.choice([31, 47, 63, 95])),\n    \"min_child_samples\": lambda: int(rng.choice([20, 40, 60, 80])),\n    \"feature_fraction\":  lambda: rng.uniform(0.7, 0.95),\n    \"bagging_fraction\":  lambda: rng.uniform(0.7, 0.95),\n    \"bagging_freq\":      lambda: int(rng.choice([1, 2, 3])),\n    \"lambda_l2\":         lambda: 10**rng.uniform(-3, 2),   # 0.001\u2013100\n}\n\n# ---- knobs you can tweak ----\nn_iter      = 200          # max trials (acts as a hard cap)\npatience    = 8            # stop if no improvement for this many trials\nmin_delta   = 1e-4         # required improvement in logloss to reset patience\ntime_limit  = 60           # seconds; e.g., 900 for 15 min, or None to disable\ntarget_loss = None         # e.g., 0.555; stop as soon as best_loss <= target_loss\n# -----------------------------\n\nbest_params, best_loss = None, float(\"inf\")\nno_improve = 0\nstart_ts = time.time()\n\nfor i in range(1, n_iter + 1):\n    # optional time budget\n    if time_limit is not None and (time.time() - start_ts) > time_limit:\n        print(f\"\u23f1\ufe0f Time budget reached at iteration {i-1}.\")\n        break\n\n    # sample a candidate\n    params = {k: f() for k, f in search_space.items()}\n\n    # evaluate via CV\n    loss = cv_score(params, X_all[keep_cols], y_all, groups, categorical_cols, n_splits=4)\n\n    # check improvement\n    if (best_loss - loss) > min_delta:\n        best_loss, best_params = loss, params\n        no_improve = 0\n        print(f\"[{i:02d}] logloss {loss:.5f} | NEW BEST | {params}\")\n    else:\n        no_improve += 1\n        print(f\"[{i:02d}] logloss {loss:.5f} | best {best_loss:.5f} | {params} | patience {no_improve}/{patience}\")\n\n    if (target_loss is not None) and (best_loss <= target_loss):\n        print(f\"\ud83c\udfaf Reached target loss {target_loss:.5f} at iter {i}.\")\n        break\n\n    # early stop when stagnating\n    if no_improve >= patience:\n        print(f\"\u2705 Early stop at iter {i} (no improvement in last {patience} trials).\")\n        break\n\nprint(\"\\nBest params:\", best_params)\nprint(\"Best CV logloss:\", best_loss)"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Warning] feature_fraction is set=0.8096023078602234, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8096023078602234"
        },
        {
          "type": "stdout",
          "content": "\n[LightGBM] [Warning] lambda_l2 is set=0.034678781225883096, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.034678781225883096\n[LightGBM] [Warning] bagging_fraction is set=0.8808662944577352, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8808662944577352\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8096023078602234, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8096023078602234\n[LightGBM] [Warning] lambda_l2 is set=0.034678781225883096, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.034678781225883096\n[LightGBM] [Warning] bagging_fraction is set=0.8808662944577352, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8808662944577352\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] Number of positive: 11821, number of negative: 8298\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013010 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 27219\n[LightGBM] [Info] Number of data points in the train set: 20119, number of used features: 263\n[LightGBM] [Warning] feature_fraction is set=0.8096023078602234, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8096023078602234\n[LightGBM] [Warning] lambda_l2 is set=0.034678781225883096, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.034678781225883096\n[LightGBM] [Warning] bagging_fraction is set=0.8808662944577352, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8808662944577352\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.587554 -> initscore=0.353863\n[LightGBM] [Info] Start training from score 0.353863\nTraining until validation scores don't improve for 150 rounds\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[526]\tvalid_0's binary_logloss: 0.562556\n[LightGBM] [Warning] feature_fraction is set=0.8096023078602234, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8096023078602234\n[LightGBM] [Warning] lambda_l2 is set=0.034678781225883096, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.034678781225883096\n[LightGBM] [Warning] bagging_fraction is set=0.8808662944577352, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8808662944577352\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8096023078602234, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8096023078602234\n[LightGBM] [Warning] lambda_l2 is set=0.034678781225883096, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.034678781225883096\n[LightGBM] [Warning] bagging_fraction is set=0.8808662944577352, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8808662944577352\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Warning] feature_fraction is set=0.8096023078602234, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8096023078602234\n[LightGBM] [Warning] lambda_l2 is set=0.034678781225883096, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.034678781225883096\n[LightGBM] [Warning] bagging_fraction is set=0.8808662944577352, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8808662944577352\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] Number of positive: 11910, number of negative: 8282\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011944 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 27187\n[LightGBM] [Info] Number of data points in the train set: 20192, number of used features: 264\n[LightGBM] [Warning] feature_fraction is set=0.8096023078602234, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8096023078602234\n[LightGBM] [Warning] lambda_l2 is set=0.034678781225883096, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.034678781225883096\n[LightGBM] [Warning] bagging_fraction is set=0.8808662944577352, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8808662944577352\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.589838 -> initscore=0.363294\n[LightGBM] [Info] Start training from score 0.363294\nTraining until validation scores don't improve for 150 rounds\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[712]\tvalid_0's binary_logloss: 0.555104\n[LightGBM] [Warning] feature_fraction is set=0.8096023078602234, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8096023078602234\n[LightGBM] [Warning] lambda_l2 is set=0.034678781225883096, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.034678781225883096\n[LightGBM] [Warning] bagging_fraction is set=0.8808662944577352, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8808662944577352\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8096023078602234, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8096023078602234\n[LightGBM] [Warning] lambda_l2 is set=0.034678781225883096, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.034678781225883096\n[LightGBM] [Warning] bagging_fraction is set=0.8808662944577352, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8808662944577352\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Warning] feature_fraction is set=0.8096023078602234, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8096023078602234\n[LightGBM] [Warning] lambda_l2 is set=0.034678781225883096, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.034678781225883096\n[LightGBM] [Warning] bagging_fraction is set=0.8808662944577352, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8808662944577352\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] Number of positive: 11860, number of negative: 8333\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014956 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 27246\n[LightGBM] [Info] Number of data points in the train set: 20193, number of used features: 264\n[LightGBM] [Warning] feature_fraction is set=0.8096023078602234, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8096023078602234\n[LightGBM] [Warning] lambda_l2 is set=0.034678781225883096, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.034678781225883096\n[LightGBM] [Warning] bagging_fraction is set=0.8808662944577352, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8808662944577352\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.587332 -> initscore=0.352948\n[LightGBM] [Info] Start training from score 0.352948\nTraining until validation scores don't improve for 150 rounds\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[512]\tvalid_0's binary_logloss: 0.572344\n[LightGBM] [Warning] feature_fraction is set=0.8096023078602234, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8096023078602234\n[LightGBM] [Warning] lambda_l2 is set=0.034678781225883096, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.034678781225883096\n[LightGBM] [Warning] bagging_fraction is set=0.8808662944577352, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8808662944577352\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.8096023078602234, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8096023078602234\n[LightGBM] [Warning] lambda_l2 is set=0.034678781225883096, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.034678781225883096\n[LightGBM] [Warning] bagging_fraction is set=0.8808662944577352, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8808662944577352\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Warning] feature_fraction is set=0.8096023078602234, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8096023078602234\n[LightGBM] [Warning] lambda_l2 is set=0.034678781225883096, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.034678781225883096\n[LightGBM] [Warning] bagging_fraction is set=0.8808662944577352, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8808662944577352\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] Number of positive: 11827, number of negative: 8366\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013410 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 27219\n[LightGBM] [Info] Number of data points in the train set: 20193, number of used features: 264\n[LightGBM] [Warning] feature_fraction is set=0.8096023078602234, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8096023078602234\n[LightGBM] [Warning] lambda_l2 is set=0.034678781225883096, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.034678781225883096\n[LightGBM] [Warning] bagging_fraction is set=0.8808662944577352, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8808662944577352\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.585698 -> initscore=0.346209\n[LightGBM] [Info] Start training from score 0.346209\nTraining until validation scores don't improve for 150 rounds\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[511]\tvalid_0's binary_logloss: 0.568485\n[LightGBM] [Warning] feature_fraction is set=0.8096023078602234, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8096023078602234\n[LightGBM] [Warning] lambda_l2 is set=0.034678781225883096, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.034678781225883096\n[LightGBM] [Warning] bagging_fraction is set=0.8808662944577352, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8808662944577352\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[01] logloss 0.56462 | NEW BEST | {'learning_rate': 0.011509239604626666, 'num_leaves': 47, 'min_child_samples': 60, 'feature_fraction': 0.8096023078602234, 'bagging_fraction': 0.8808662944577352, 'bagging_freq': 1, 'lambda_l2': 0.034678781225883096}\n[LightGBM] [Warning] feature_fraction is set=0.804843052604122, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.804843052604122\n[LightGBM] [Warning] lambda_l2 is set=2.4897169764256213, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4897169764256213\n[LightGBM] [Warning] bagging_fraction is set=0.7039775902403191, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7039775902403191\n[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Warning] feature_fraction is set=0.804843052604122, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.804843052604122\n[LightGBM] [Warning] lambda_l2 is set=2.4897169764256213, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4897169764256213\n[LightGBM] [Warning] bagging_fraction is set=0.7039775902403191, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7039775902403191\n[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n[LightGBM] [Info] Number of positive: 11821, number of negative: 8298\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012353 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 27219\n[LightGBM] [Info] Number of data points in the train set: 20119, number of used features: 263\n[LightGBM] [Warning] feature_fraction is set=0.804843052604122, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.804843052604122\n[LightGBM] [Warning] lambda_l2 is set=2.4897169764256213, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4897169764256213\n[LightGBM] [Warning] bagging_fraction is set=0.7039775902403191, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7039775902403191\n[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.587554 -> initscore=0.353863\n[LightGBM] [Info] Start training from score 0.353863\nTraining until validation scores don't improve for 150 rounds\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[436]\tvalid_0's binary_logloss: 0.563942\n[LightGBM] [Warning] feature_fraction is set=0.804843052604122, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.804843052604122\n[LightGBM] [Warning] lambda_l2 is set=2.4897169764256213, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4897169764256213\n[LightGBM] [Warning] bagging_fraction is set=0.7039775902403191, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7039775902403191\n[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n[LightGBM] [Warning] feature_fraction is set=0.804843052604122, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.804843052604122\n[LightGBM] [Warning] lambda_l2 is set=2.4897169764256213, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4897169764256213\n[LightGBM] [Warning] bagging_fraction is set=0.7039775902403191, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7039775902403191\n[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Warning] feature_fraction is set=0.804843052604122, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.804843052604122\n[LightGBM] [Warning] lambda_l2 is set=2.4897169764256213, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4897169764256213\n[LightGBM] [Warning] bagging_fraction is set=0.7039775902403191, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7039775902403191\n[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n[LightGBM] [Info] Number of positive: 11910, number of negative: 8282\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013408 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 27187\n[LightGBM] [Info] Number of data points in the train set: 20192, number of used features: 264\n[LightGBM] [Warning] feature_fraction is set=0.804843052604122, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.804843052604122\n[LightGBM] [Warning] lambda_l2 is set=2.4897169764256213, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4897169764256213\n[LightGBM] [Warning] bagging_fraction is set=0.7039775902403191, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7039775902403191\n[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.589838 -> initscore=0.363294\n[LightGBM] [Info] Start training from score 0.363294\nTraining until validation scores don't improve for 150 rounds\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[466]\tvalid_0's binary_logloss: 0.557454\n[LightGBM] [Warning] feature_fraction is set=0.804843052604122, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.804843052604122\n[LightGBM] [Warning] lambda_l2 is set=2.4897169764256213, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4897169764256213\n[LightGBM] [Warning] bagging_fraction is set=0.7039775902403191, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7039775902403191\n[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n[LightGBM] [Warning] feature_fraction is set=0.804843052604122, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.804843052604122\n[LightGBM] [Warning] lambda_l2 is set=2.4897169764256213, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4897169764256213\n[LightGBM] [Warning] bagging_fraction is set=0.7039775902403191, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7039775902403191\n[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Warning] feature_fraction is set=0.804843052604122, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.804843052604122\n[LightGBM] [Warning] lambda_l2 is set=2.4897169764256213, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4897169764256213\n[LightGBM] [Warning] bagging_fraction is set=0.7039775902403191, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7039775902403191\n[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n[LightGBM] [Info] Number of positive: 11860, number of negative: 8333\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016666 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 27246\n[LightGBM] [Info] Number of data points in the train set: 20193, number of used features: 264\n[LightGBM] [Warning] feature_fraction is set=0.804843052604122, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.804843052604122\n[LightGBM] [Warning] lambda_l2 is set=2.4897169764256213, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4897169764256213\n[LightGBM] [Warning] bagging_fraction is set=0.7039775902403191, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7039775902403191\n[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.587332 -> initscore=0.352948\n[LightGBM] [Info] Start training from score 0.352948\nTraining until validation scores don't improve for 150 rounds\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[348]\tvalid_0's binary_logloss: 0.573546\n[LightGBM] [Warning] feature_fraction is set=0.804843052604122, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.804843052604122\n[LightGBM] [Warning] lambda_l2 is set=2.4897169764256213, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4897169764256213\n[LightGBM] [Warning] bagging_fraction is set=0.7039775902403191, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7039775902403191\n[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n[LightGBM] [Warning] feature_fraction is set=0.804843052604122, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.804843052604122\n[LightGBM] [Warning] lambda_l2 is set=2.4897169764256213, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4897169764256213\n[LightGBM] [Warning] bagging_fraction is set=0.7039775902403191, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7039775902403191\n[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Warning] feature_fraction is set=0.804843052604122, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.804843052604122\n[LightGBM] [Warning] lambda_l2 is set=2.4897169764256213, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4897169764256213\n[LightGBM] [Warning] bagging_fraction is set=0.7039775902403191, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7039775902403191\n[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n[LightGBM] [Info] Number of positive: 11827, number of negative: 8366\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014503 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 27219\n[LightGBM] [Info] Number of data points in the train set: 20193, number of used features: 264\n[LightGBM] [Warning] feature_fraction is set=0.804843052604122, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.804843052604122\n[LightGBM] [Warning] lambda_l2 is set=2.4897169764256213, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4897169764256213\n[LightGBM] [Warning] bagging_fraction is set=0.7039775902403191, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7039775902403191\n[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.585698 -> initscore=0.346209\n[LightGBM] [Info] Start training from score 0.346209\nTraining until validation scores don't improve for 150 rounds\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[357]\tvalid_0's binary_logloss: 0.570489\n[LightGBM] [Warning] feature_fraction is set=0.804843052604122, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.804843052604122\n[LightGBM] [Warning] lambda_l2 is set=2.4897169764256213, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4897169764256213\n[LightGBM] [Warning] bagging_fraction is set=0.7039775902403191, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7039775902403191\n[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n[02] logloss 0.56636 | best 0.56462 | {'learning_rate': 0.016259106329918963, 'num_leaves': 47, 'min_child_samples': 60, 'feature_fraction': 0.804843052604122, 'bagging_fraction': 0.7039775902403191, 'bagging_freq': 3, 'lambda_l2': 2.4897169764256213} | patience 1/8\n[LightGBM] [Warning] feature_fraction is set=0.7164840867264762, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7164840867264762\n[LightGBM] [Warning] lambda_l2 is set=0.011666128825437919, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011666128825437919\n[LightGBM] [Warning] bagging_fraction is set=0.7720363998269983, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7720363998269983\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Warning] feature_fraction is set=0.7164840867264762, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7164840867264762\n[LightGBM] [Warning] lambda_l2 is set=0.011666128825437919, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011666128825437919\n[LightGBM] [Warning] bagging_fraction is set=0.7720363998269983, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7720363998269983\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] Number of positive: 11821, number of negative: 8298\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013823 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 27226\n[LightGBM] [Info] Number of data points in the train set: 20119, number of used features: 266\n[LightGBM] [Warning] feature_fraction is set=0.7164840867264762, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7164840867264762\n[LightGBM] [Warning] lambda_l2 is set=0.011666128825437919, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011666128825437919\n[LightGBM] [Warning] bagging_fraction is set=0.7720363998269983, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7720363998269983\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.587554 -> initscore=0.353863\n[LightGBM] [Info] Start training from score 0.353863\nTraining until validation scores don't improve for 150 rounds\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[211]\tvalid_0's binary_logloss: 0.563811\n[LightGBM] [Warning] feature_fraction is set=0.7164840867264762, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7164840867264762\n[LightGBM] [Warning] lambda_l2 is set=0.011666128825437919, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011666128825437919\n[LightGBM] [Warning] bagging_fraction is set=0.7720363998269983, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7720363998269983\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.7164840867264762, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7164840867264762\n[LightGBM] [Warning] lambda_l2 is set=0.011666128825437919, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011666128825437919\n[LightGBM] [Warning] bagging_fraction is set=0.7720363998269983, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7720363998269983\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Warning] feature_fraction is set=0.7164840867264762, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7164840867264762\n[LightGBM] [Warning] lambda_l2 is set=0.011666128825437919, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011666128825437919\n[LightGBM] [Warning] bagging_fraction is set=0.7720363998269983, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7720363998269983\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] Number of positive: 11910, number of negative: 8282\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012816 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 27194\n[LightGBM] [Info] Number of data points in the train set: 20192, number of used features: 267\n[LightGBM] [Warning] feature_fraction is set=0.7164840867264762, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7164840867264762\n[LightGBM] [Warning] lambda_l2 is set=0.011666128825437919, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011666128825437919\n[LightGBM] [Warning] bagging_fraction is set=0.7720363998269983, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7720363998269983\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.589838 -> initscore=0.363294\n[LightGBM] [Info] Start training from score 0.363294\nTraining until validation scores don't improve for 150 rounds\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[169]\tvalid_0's binary_logloss: 0.556901\n[LightGBM] [Warning] feature_fraction is set=0.7164840867264762, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7164840867264762\n[LightGBM] [Warning] lambda_l2 is set=0.011666128825437919, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011666128825437919\n[LightGBM] [Warning] bagging_fraction is set=0.7720363998269983, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7720363998269983\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.7164840867264762, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7164840867264762\n[LightGBM] [Warning] lambda_l2 is set=0.011666128825437919, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011666128825437919\n[LightGBM] [Warning] bagging_fraction is set=0.7720363998269983, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7720363998269983\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Warning] feature_fraction is set=0.7164840867264762, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7164840867264762\n[LightGBM] [Warning] lambda_l2 is set=0.011666128825437919, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011666128825437919\n[LightGBM] [Warning] bagging_fraction is set=0.7720363998269983, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7720363998269983\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] Number of positive: 11860, number of negative: 8333\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012679 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 27253\n[LightGBM] [Info] Number of data points in the train set: 20193, number of used features: 267\n[LightGBM] [Warning] feature_fraction is set=0.7164840867264762, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7164840867264762\n[LightGBM] [Warning] lambda_l2 is set=0.011666128825437919, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011666128825437919\n[LightGBM] [Warning] bagging_fraction is set=0.7720363998269983, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7720363998269983\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.587332 -> initscore=0.352948\n[LightGBM] [Info] Start training from score 0.352948\nTraining until validation scores don't improve for 150 rounds\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[163]\tvalid_0's binary_logloss: 0.571947\n[LightGBM] [Warning] feature_fraction is set=0.7164840867264762, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7164840867264762\n[LightGBM] [Warning] lambda_l2 is set=0.011666128825437919, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011666128825437919\n[LightGBM] [Warning] bagging_fraction is set=0.7720363998269983, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7720363998269983\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.7164840867264762, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7164840867264762\n[LightGBM] [Warning] lambda_l2 is set=0.011666128825437919, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011666128825437919\n[LightGBM] [Warning] bagging_fraction is set=0.7720363998269983, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7720363998269983\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Warning] feature_fraction is set=0.7164840867264762, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7164840867264762\n[LightGBM] [Warning] lambda_l2 is set=0.011666128825437919, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011666128825437919\n[LightGBM] [Warning] bagging_fraction is set=0.7720363998269983, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7720363998269983\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] Number of positive: 11827, number of negative: 8366\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012340 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 27226\n[LightGBM] [Info] Number of data points in the train set: 20193, number of used features: 267\n[LightGBM] [Warning] feature_fraction is set=0.7164840867264762, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7164840867264762\n[LightGBM] [Warning] lambda_l2 is set=0.011666128825437919, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011666128825437919\n[LightGBM] [Warning] bagging_fraction is set=0.7720363998269983, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7720363998269983\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.585698 -> initscore=0.346209\n[LightGBM] [Info] Start training from score 0.346209\nTraining until validation scores don't improve for 150 rounds\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[163]\tvalid_0's binary_logloss: 0.570993\n[LightGBM] [Warning] feature_fraction is set=0.7164840867264762, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7164840867264762\n[LightGBM] [Warning] lambda_l2 is set=0.011666128825437919, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.011666128825437919\n[LightGBM] [Warning] bagging_fraction is set=0.7720363998269983, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7720363998269983\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[03] logloss 0.56591 | best 0.56462 | {'learning_rate': 0.043953273843858706, 'num_leaves': 31, 'min_child_samples': 20, 'feature_fraction': 0.7164840867264762, 'bagging_fraction': 0.7720363998269983, 'bagging_freq': 1, 'lambda_l2': 0.011666128825437919} | patience 2/8\n[LightGBM] [Warning] feature_fraction is set=0.7062248068875869, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7062248068875869\n[LightGBM] [Warning] lambda_l2 is set=18.04982233305922, reg_lambda=0.0 will be ignored. Current value: lambda_l2=18.04982233305922\n[LightGBM] [Warning] bagging_fraction is set=0.8501372293660305, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8501372293660305\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Warning] feature_fraction is set=0.7062248068875869, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7062248068875869\n[LightGBM] [Warning] lambda_l2 is set=18.04982233305922, reg_lambda=0.0 will be ignored. Current value: lambda_l2=18.04982233305922\n[LightGBM] [Warning] bagging_fraction is set=0.8501372293660305, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8501372293660305\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] Number of positive: 11821, number of negative: 8298\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015700 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 27217\n[LightGBM] [Info] Number of data points in the train set: 20119, number of used features: 262\n[LightGBM] [Warning] feature_fraction is set=0.7062248068875869, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7062248068875869\n[LightGBM] [Warning] lambda_l2 is set=18.04982233305922, reg_lambda=0.0 will be ignored. Current value: lambda_l2=18.04982233305922\n[LightGBM] [Warning] bagging_fraction is set=0.8501372293660305, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8501372293660305\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.587554 -> initscore=0.353863\n[LightGBM] [Info] Start training from score 0.353863\nTraining until validation scores don't improve for 150 rounds\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[547]\tvalid_0's binary_logloss: 0.56427\n[LightGBM] [Warning] feature_fraction is set=0.7062248068875869, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7062248068875869\n[LightGBM] [Warning] lambda_l2 is set=18.04982233305922, reg_lambda=0.0 will be ignored. Current value: lambda_l2=18.04982233305922\n[LightGBM] [Warning] bagging_fraction is set=0.8501372293660305, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8501372293660305\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.7062248068875869, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7062248068875869\n[LightGBM] [Warning] lambda_l2 is set=18.04982233305922, reg_lambda=0.0 will be ignored. Current value: lambda_l2=18.04982233305922\n[LightGBM] [Warning] bagging_fraction is set=0.8501372293660305, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8501372293660305\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Warning] feature_fraction is set=0.7062248068875869, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7062248068875869\n[LightGBM] [Warning] lambda_l2 is set=18.04982233305922, reg_lambda=0.0 will be ignored. Current value: lambda_l2=18.04982233305922\n[LightGBM] [Warning] bagging_fraction is set=0.8501372293660305, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8501372293660305\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] Number of positive: 11910, number of negative: 8282\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013892 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 27185\n[LightGBM] [Info] Number of data points in the train set: 20192, number of used features: 263\n[LightGBM] [Warning] feature_fraction is set=0.7062248068875869, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7062248068875869\n[LightGBM] [Warning] lambda_l2 is set=18.04982233305922, reg_lambda=0.0 will be ignored. Current value: lambda_l2=18.04982233305922\n[LightGBM] [Warning] bagging_fraction is set=0.8501372293660305, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8501372293660305\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.589838 -> initscore=0.363294\n[LightGBM] [Info] Start training from score 0.363294\nTraining until validation scores don't improve for 150 rounds\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[415]\tvalid_0's binary_logloss: 0.55822\n[LightGBM] [Warning] feature_fraction is set=0.7062248068875869, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7062248068875869\n[LightGBM] [Warning] lambda_l2 is set=18.04982233305922, reg_lambda=0.0 will be ignored. Current value: lambda_l2=18.04982233305922\n[LightGBM] [Warning] bagging_fraction is set=0.8501372293660305, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8501372293660305\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.7062248068875869, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7062248068875869\n[LightGBM] [Warning] lambda_l2 is set=18.04982233305922, reg_lambda=0.0 will be ignored. Current value: lambda_l2=18.04982233305922\n[LightGBM] [Warning] bagging_fraction is set=0.8501372293660305, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8501372293660305\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Warning] feature_fraction is set=0.7062248068875869, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7062248068875869\n[LightGBM] [Warning] lambda_l2 is set=18.04982233305922, reg_lambda=0.0 will be ignored. Current value: lambda_l2=18.04982233305922\n[LightGBM] [Warning] bagging_fraction is set=0.8501372293660305, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8501372293660305\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] Number of positive: 11860, number of negative: 8333\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014031 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 27244\n[LightGBM] [Info] Number of data points in the train set: 20193, number of used features: 263\n[LightGBM] [Warning] feature_fraction is set=0.7062248068875869, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7062248068875869\n[LightGBM] [Warning] lambda_l2 is set=18.04982233305922, reg_lambda=0.0 will be ignored. Current value: lambda_l2=18.04982233305922\n[LightGBM] [Warning] bagging_fraction is set=0.8501372293660305, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8501372293660305\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.587332 -> initscore=0.352948\n[LightGBM] [Info] Start training from score 0.352948\nTraining until validation scores don't improve for 150 rounds\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[339]\tvalid_0's binary_logloss: 0.574781\n[LightGBM] [Warning] feature_fraction is set=0.7062248068875869, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7062248068875869\n[LightGBM] [Warning] lambda_l2 is set=18.04982233305922, reg_lambda=0.0 will be ignored. Current value: lambda_l2=18.04982233305922\n[LightGBM] [Warning] bagging_fraction is set=0.8501372293660305, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8501372293660305\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.7062248068875869, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7062248068875869\n[LightGBM] [Warning] lambda_l2 is set=18.04982233305922, reg_lambda=0.0 will be ignored. Current value: lambda_l2=18.04982233305922\n[LightGBM] [Warning] bagging_fraction is set=0.8501372293660305, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8501372293660305\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Warning] feature_fraction is set=0.7062248068875869, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7062248068875869\n[LightGBM] [Warning] lambda_l2 is set=18.04982233305922, reg_lambda=0.0 will be ignored. Current value: lambda_l2=18.04982233305922\n[LightGBM] [Warning] bagging_fraction is set=0.8501372293660305, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8501372293660305\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] Number of positive: 11827, number of negative: 8366\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013638 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 27217\n[LightGBM] [Info] Number of data points in the train set: 20193, number of used features: 263\n[LightGBM] [Warning] feature_fraction is set=0.7062248068875869, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7062248068875869\n[LightGBM] [Warning] lambda_l2 is set=18.04982233305922, reg_lambda=0.0 will be ignored. Current value: lambda_l2=18.04982233305922\n[LightGBM] [Warning] bagging_fraction is set=0.8501372293660305, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8501372293660305\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.585698 -> initscore=0.346209\n[LightGBM] [Info] Start training from score 0.346209\nTraining until validation scores don't improve for 150 rounds\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[382]\tvalid_0's binary_logloss: 0.570687\n[LightGBM] [Warning] feature_fraction is set=0.7062248068875869, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7062248068875869\n[LightGBM] [Warning] lambda_l2 is set=18.04982233305922, reg_lambda=0.0 will be ignored. Current value: lambda_l2=18.04982233305922\n[LightGBM] [Warning] bagging_fraction is set=0.8501372293660305, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8501372293660305\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[04] logloss 0.56699 | best 0.56462 | {'learning_rate': 0.022998481884465672, 'num_leaves': 31, 'min_child_samples': 80, 'feature_fraction': 0.7062248068875869, 'bagging_fraction': 0.8501372293660305, 'bagging_freq': 1, 'lambda_l2': 18.04982233305922} | patience 3/8\n[LightGBM] [Warning] feature_fraction is set=0.9344073339051928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344073339051928\n[LightGBM] [Warning] lambda_l2 is set=0.06128810667082612, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06128810667082612\n[LightGBM] [Warning] bagging_fraction is set=0.8122064488444711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8122064488444711\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Warning] feature_fraction is set=0.9344073339051928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344073339051928\n[LightGBM] [Warning] lambda_l2 is set=0.06128810667082612, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06128810667082612\n[LightGBM] [Warning] bagging_fraction is set=0.8122064488444711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8122064488444711\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] Number of positive: 11821, number of negative: 8298\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013151 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 27226\n[LightGBM] [Info] Number of data points in the train set: 20119, number of used features: 266\n[LightGBM] [Warning] feature_fraction is set=0.9344073339051928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344073339051928\n[LightGBM] [Warning] lambda_l2 is set=0.06128810667082612, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06128810667082612\n[LightGBM] [Warning] bagging_fraction is set=0.8122064488444711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8122064488444711\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.587554 -> initscore=0.353863\n[LightGBM] [Info] Start training from score 0.353863\nTraining until validation scores don't improve for 150 rounds\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[441]\tvalid_0's binary_logloss: 0.562176\n[LightGBM] [Warning] feature_fraction is set=0.9344073339051928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344073339051928\n[LightGBM] [Warning] lambda_l2 is set=0.06128810667082612, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06128810667082612\n[LightGBM] [Warning] bagging_fraction is set=0.8122064488444711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8122064488444711\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.9344073339051928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344073339051928\n[LightGBM] [Warning] lambda_l2 is set=0.06128810667082612, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06128810667082612\n[LightGBM] [Warning] bagging_fraction is set=0.8122064488444711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8122064488444711\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Warning] feature_fraction is set=0.9344073339051928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344073339051928\n[LightGBM] [Warning] lambda_l2 is set=0.06128810667082612, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06128810667082612\n[LightGBM] [Warning] bagging_fraction is set=0.8122064488444711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8122064488444711\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] Number of positive: 11910, number of negative: 8282\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010837 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 27194\n[LightGBM] [Info] Number of data points in the train set: 20192, number of used features: 267\n[LightGBM] [Warning] feature_fraction is set=0.9344073339051928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344073339051928\n[LightGBM] [Warning] lambda_l2 is set=0.06128810667082612, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06128810667082612\n[LightGBM] [Warning] bagging_fraction is set=0.8122064488444711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8122064488444711\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.589838 -> initscore=0.363294\n[LightGBM] [Info] Start training from score 0.363294\nTraining until validation scores don't improve for 150 rounds\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[416]\tvalid_0's binary_logloss: 0.554834\n[LightGBM] [Warning] feature_fraction is set=0.9344073339051928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344073339051928\n[LightGBM] [Warning] lambda_l2 is set=0.06128810667082612, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06128810667082612\n[LightGBM] [Warning] bagging_fraction is set=0.8122064488444711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8122064488444711\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.9344073339051928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344073339051928\n[LightGBM] [Warning] lambda_l2 is set=0.06128810667082612, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06128810667082612\n[LightGBM] [Warning] bagging_fraction is set=0.8122064488444711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8122064488444711\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Warning] feature_fraction is set=0.9344073339051928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344073339051928\n[LightGBM] [Warning] lambda_l2 is set=0.06128810667082612, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06128810667082612\n[LightGBM] [Warning] bagging_fraction is set=0.8122064488444711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8122064488444711\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] Number of positive: 11860, number of negative: 8333\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012065 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 27253\n[LightGBM] [Info] Number of data points in the train set: 20193, number of used features: 267\n[LightGBM] [Warning] feature_fraction is set=0.9344073339051928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344073339051928\n[LightGBM] [Warning] lambda_l2 is set=0.06128810667082612, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06128810667082612\n[LightGBM] [Warning] bagging_fraction is set=0.8122064488444711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8122064488444711\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.587332 -> initscore=0.352948\n[LightGBM] [Info] Start training from score 0.352948\nTraining until validation scores don't improve for 150 rounds\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[318]\tvalid_0's binary_logloss: 0.57127\n[LightGBM] [Warning] feature_fraction is set=0.9344073339051928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344073339051928\n[LightGBM] [Warning] lambda_l2 is set=0.06128810667082612, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06128810667082612\n[LightGBM] [Warning] bagging_fraction is set=0.8122064488444711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8122064488444711\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.9344073339051928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344073339051928\n[LightGBM] [Warning] lambda_l2 is set=0.06128810667082612, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06128810667082612\n[LightGBM] [Warning] bagging_fraction is set=0.8122064488444711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8122064488444711\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Warning] feature_fraction is set=0.9344073339051928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344073339051928\n[LightGBM] [Warning] lambda_l2 is set=0.06128810667082612, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06128810667082612\n[LightGBM] [Warning] bagging_fraction is set=0.8122064488444711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8122064488444711\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] Number of positive: 11827, number of negative: 8366\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016659 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 27226\n[LightGBM] [Info] Number of data points in the train set: 20193, number of used features: 267\n[LightGBM] [Warning] feature_fraction is set=0.9344073339051928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344073339051928\n[LightGBM] [Warning] lambda_l2 is set=0.06128810667082612, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06128810667082612\n[LightGBM] [Warning] bagging_fraction is set=0.8122064488444711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8122064488444711\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.585698 -> initscore=0.346209\n[LightGBM] [Info] Start training from score 0.346209\nTraining until validation scores don't improve for 150 rounds\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[417]\tvalid_0's binary_logloss: 0.568563\n[LightGBM] [Warning] feature_fraction is set=0.9344073339051928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344073339051928\n[LightGBM] [Warning] lambda_l2 is set=0.06128810667082612, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06128810667082612\n[LightGBM] [Warning] bagging_fraction is set=0.8122064488444711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8122064488444711\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[05] logloss 0.56421 | NEW BEST | {'learning_rate': 0.013779777694590706, 'num_leaves': 63, 'min_child_samples': 20, 'feature_fraction': 0.9344073339051928, 'bagging_fraction': 0.8122064488444711, 'bagging_freq': 1, 'lambda_l2': 0.06128810667082612}\n\u23f1\ufe0f Time budget reached at iteration 5.\n\nBest params: {'learning_rate': 0.013779777694590706, 'num_leaves': 63, 'min_child_samples': 20, 'feature_fraction': 0.9344073339051928, 'bagging_fraction': 0.8122064488444711, 'bagging_freq': 1, 'lambda_l2': 0.06128810667082612}\nBest CV logloss: 0.5642107450852824\n"
        },
        {
          "type": "code",
          "language": "python",
          "content": "# same as before\ntrain_idx, val_idx, test_idx = group_train_val_test_split(\n    groups, test_size=0.2, val_size=0.2, random_state=42\n)\ntrval_idx = np.concatenate([train_idx, val_idx])\n\nX_trval_raw = X_all.loc[X_all.index[trval_idx], keep_cols].copy()\ny_trval     = y_all.iloc[trval_idx].copy()\nX_test_raw  = X_all.loc[X_all.index[test_idx],  keep_cols].copy()\ny_test      = y_all.iloc[test_idx].copy()\n\nfor df_split in (X_trval_raw, X_test_raw):\n    for c in df_split.select_dtypes(include=[\"object\"]).columns:\n        df_split[c] = df_split[c].astype(\"category\")\ncategorical_cols = list(X_trval_raw.select_dtypes(include=[\"category\"]).columns)\nfor c in categorical_cols:\n    cats = X_trval_raw[c].cat.categories.union(X_test_raw[c].cat.categories)\n    X_trval_raw[c] = X_trval_raw[c].cat.set_categories(cats)\n    X_test_raw[c]  = X_test_raw[c].cat.set_categories(cats)\n\ngroups_trval = groups.iloc[trval_idx].reset_index(drop=True)\nX_trval_raw  = X_trval_raw.reset_index(drop=True)\ny_trval      = y_trval.reset_index(drop=True)\n\ngss = GroupShuffleSplit(n_splits=1, test_size=0.10, random_state=123)\nsub_train_rel, sub_val_rel = next(gss.split(np.arange(len(groups_trval)), groups=groups_trval))\nX_subtr, y_subtr = X_trval_raw.iloc[sub_train_rel], y_trval.iloc[sub_train_rel]\nX_subva, y_subva = X_trval_raw.iloc[sub_val_rel],  y_trval.iloc[sub_val_rel]\n\nfinal_clf = LGBMClassifier(\n    n_estimators=4000,\n    random_state=42,\n    n_jobs=-1,\n    **best_params\n)\nfinal_clf.fit(\n    X_subtr, y_subtr,\n    eval_set=[(X_subva, y_subva)],\n    eval_metric=\"logloss\",\n    categorical_feature=categorical_cols,\n    callbacks=[early_stopping(150), log_evaluation(100)]\n)\nbest_iter = final_clf.best_iteration_\n\np_val = final_clf.predict_proba(X_subva, num_iteration=best_iter)[:,1]\nthr_grid = np.linspace(0.30, 0.70, 81)\nthr = max(thr_grid, key=lambda t: accuracy_score(y_subva, (p_val >= t).astype(int)))\nprint(\"Chosen threshold:\", thr, \"| best_iter:\", best_iter)\n\np_test = final_clf.predict_proba(X_test_raw, num_iteration=best_iter)[:,1]\ny_hat  = (p_test >= thr).astype(int)\nprint(\n    \"TEST  Acc:\", accuracy_score(y_test, y_hat),\n    \"| AUC:\", roc_auc_score(y_test, p_test),\n    \"| LogLoss:\", log_loss(y_test, p_test),\n    \"| Brier:\", brier_score_loss(y_test, p_test)\n)\nprint(\"TEST CM:\\n\", confusion_matrix(y_test, y_hat))"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Warning] feature_fraction is set=0.9344073339051928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344073339051928\n[LightGBM] [Warning] lambda_l2 is set=0.06128810667082612, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06128810667082612\n[LightGBM] [Warning] bagging_fraction is set=0.8122064488444711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8122064488444711\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.9344073339051928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344073339051928\n[LightGBM] [Warning] lambda_l2 is set=0.06128810667082612, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06128810667082612\n[LightGBM] [Warning] bagging_fraction is set=0.8122064488444711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8122064488444711\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] Number of positive: 11350, number of negative: 7955\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011879 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 27231\n[LightGBM] [Info] Number of data points in the train set: 19305, number of used features: 266\n[LightGBM] [Warning] feature_fraction is set=0.9344073339051928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344073339051928\n[LightGBM] [Warning] lambda_l2 is set=0.06128810667082612, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06128810667082612\n[LightGBM] [Warning] bagging_fraction is set=0.8122064488444711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8122064488444711\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.587931 -> initscore=0.355417\n[LightGBM] [Info] Start training from score 0.355417\nTraining until validation scores don't improve for 150 rounds\n"
        },
        {
          "type": "stdout",
          "content": "[100]\tvalid_0's binary_logloss: 0.581053\n"
        },
        {
          "type": "stdout",
          "content": "[200]\tvalid_0's binary_logloss: 0.562212\n"
        },
        {
          "type": "stdout",
          "content": "[300]\tvalid_0's binary_logloss: 0.558051\n"
        },
        {
          "type": "stdout",
          "content": "[400]\tvalid_0's binary_logloss: 0.556046\n"
        },
        {
          "type": "stdout",
          "content": "[500]\tvalid_0's binary_logloss: 0.556134\n"
        },
        {
          "type": "stdout",
          "content": "Early stopping, best iteration is:\n[422]\tvalid_0's binary_logloss: 0.555759\n[LightGBM] [Warning] feature_fraction is set=0.9344073339051928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344073339051928\n[LightGBM] [Warning] lambda_l2 is set=0.06128810667082612, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06128810667082612\n[LightGBM] [Warning] bagging_fraction is set=0.8122064488444711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8122064488444711\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nChosen threshold: 0.5249999999999999 | best_iter: 422\n[LightGBM] [Warning] feature_fraction is set=0.9344073339051928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344073339051928\n[LightGBM] [Warning] lambda_l2 is set=0.06128810667082612, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06128810667082612\n[LightGBM] [Warning] bagging_fraction is set=0.8122064488444711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8122064488444711\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\nTEST  Acc: 0.7076099002585888 | AUC: 0.7682598596186605 | LogLoss: 0.5619005797058109 | Brier: 0.19061293233966448\nTEST CM:\n [[1397  791]\n [ 792 2434]]\n"
        },
        {
          "type": "code",
          "language": "python",
          "content": "# Recompute fold-val/test probabilities just to be explicit\np_val  = final_clf.predict_proba(X_subva,  num_iteration=best_iter)[:, 1]\np_test = final_clf.predict_proba(X_test_raw, num_iteration=best_iter)[:, 1]\n\n# Fit isotonic on sub-val; transform test\niso = IsotonicRegression(out_of_bounds=\"clip\")\niso.fit(p_val, y_subva)\np_test_cal = iso.transform(p_test)\n\ndef expected_calibration_error(y_true, p, n_bins=15):\n    bins = np.linspace(0.0, 1.0, n_bins + 1)\n    idx  = np.digitize(p, bins) - 1\n    ece  = 0.0\n    n    = len(p)\n    for b in range(n_bins):\n        mask = idx == b\n        if mask.sum() == 0: \n            continue\n        gap = abs(p[mask].mean() - y_true[mask].mean())\n        ece += (mask.sum() / n) * gap\n    return float(ece)\n\ndef reliability_table(y_true, p, n_bins=10):\n    bins = np.linspace(0, 1, n_bins + 1)\n    idx  = np.digitize(p, bins) - 1\n    rows = []\n    for b in range(n_bins):\n        m = idx == b\n        if m.sum() == 0:\n            rows.append((f\"[{bins[b]:.2f},{bins[b+1]:.2f})\", 0, np.nan, np.nan, np.nan))\n        else:\n            rows.append((\n                f\"[{bins[b]:.2f},{bins[b+1]:.2f})\",\n                int(m.sum()),\n                float(p[m].mean()),\n                float(y_true[m].mean()),\n                float(abs(p[m].mean() - y_true[m].mean()))\n            ))\n    return pd.DataFrame(rows, columns=[\"bin\",\"count\",\"mean_pred\",\"frac_pos\",\"|gap|\"])\n\n# Pre-calibration vs post-calibration on TEST\ndef summarize(y, p, tag):\n    yhat = (p >= 0.55).astype(int)  # use your chosen ~0.55 thr; tweak if you want\n    print(f\"{tag:>10} | Acc {accuracy_score(y,yhat):.4f} \"\n          f\"| AUC {roc_auc_score(y,p):.4f} \"\n          f\"| LogLoss {log_loss(y,p):.4f} \"\n          f\"| Brier {brier_score_loss(y,p):.4f} \"\n          f\"| ECE {expected_calibration_error(y,p):.4f}\")\n\nsummarize(y_test, p_test,      \"RAW\")\nsummarize(y_test, p_test_cal,  \"CAL\")\n\nrel_raw = reliability_table(y_test, p_test, n_bins=10)\nrel_cal = reliability_table(y_test, p_test_cal, n_bins=10)\nprint(\"\\nReliability (raw):\\n\", rel_raw)\nprint(\"\\nReliability (cal):\\n\", rel_cal)"
        },
        {
          "type": "stdout",
          "content": "[LightGBM] [Warning] feature_fraction is set=0.9344073339051928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344073339051928\n[LightGBM] [Warning] lambda_l2 is set=0.06128810667082612, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06128810667082612\n[LightGBM] [Warning] bagging_fraction is set=0.8122064488444711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8122064488444711\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] feature_fraction is set=0.9344073339051928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344073339051928\n[LightGBM] [Warning] lambda_l2 is set=0.06128810667082612, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06128810667082612\n[LightGBM] [Warning] bagging_fraction is set=0.8122064488444711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8122064488444711\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n       RAW | Acc 0.7041 | AUC 0.7683 | LogLoss 0.5619 | Brier 0.1906 | ECE 0.0215\n       CAL | Acc 0.6917 | AUC 0.7656 | LogLoss 0.5788 | Brier 0.1925 | ECE 0.0375\n\nReliability (raw):\n            bin  count  mean_pred  frac_pos     |gap|\n0  [0.00,0.10)     17   0.073926  0.000000  0.073926\n1  [0.10,0.20)    143   0.171848  0.195804  0.023957\n2  [0.20,0.30)    471   0.255082  0.267516  0.012434\n3  [0.30,0.40)    652   0.352482  0.360429  0.007947\n4  [0.40,0.50)    724   0.451070  0.437845  0.013224\n5  [0.50,0.60)    685   0.547778  0.551825  0.004047\n6  [0.60,0.70)    602   0.650072  0.606312  0.043759\n7  [0.70,0.80)    761   0.753252  0.775296  0.022044\n8  [0.80,0.90)   1122   0.851245  0.858289  0.007044\n9  [0.90,1.00)    237   0.916193  0.945148  0.028954\n\nReliability (cal):\n            bin  count  mean_pred  frac_pos     |gap|\n0  [0.00,0.10)     46   0.054300  0.130435  0.076135\n1  [0.10,0.20)    149   0.155816  0.208054  0.052238\n2  [0.20,0.30)    686   0.243412  0.287172  0.043760\n3  [0.30,0.40)    559   0.334096  0.373882  0.039786\n4  [0.40,0.50)    758   0.424907  0.465699  0.040792\n5  [0.50,0.60)    541   0.514496  0.578558  0.064062\n6  [0.60,0.70)    599   0.668080  0.612688  0.055393\n7  [0.70,0.80)    919   0.768487  0.797606  0.029119\n8  [0.80,0.90)    862   0.850463  0.858469  0.008005\n9  [0.90,1.00)    226   0.969441  0.929204  0.040237\n"
        }
      ]
    },
    {
      "index": 6,
      "title": "Improvements from 1.0",
      "blocks": [
        {
          "type": "code",
          "language": "python",
          "content": "plt.figure(figsize=(6,5))\n\n# New (raw)\nfpr_new, tpr_new, _ = roc_curve(y_test, p_test)\nauc_new = auc(fpr_new, tpr_new)\nplt.plot(fpr_new, tpr_new, label=f\"New (raw) AUC={auc_new:.3f}\")\n\n# New (cal)\nfpr_cal, tpr_cal, _ = roc_curve(y_test, p_test_cal)\nauc_cal = auc(fpr_cal, tpr_cal)\nplt.plot(fpr_cal, tpr_cal, label=f\"New (cal) AUC={auc_cal:.3f}\")\n\n# Baseline (if available)\nif \"p_test_base\" in globals():\n    fpr_b, tpr_b, _ = roc_curve(y_test, p_test_base)\n    auc_b = auc(fpr_b, tpr_b)\n    plt.plot(fpr_b, tpr_b, label=f\"Baseline AUC={auc_b:.3f}\")\n\n# Diagonal\nplt.plot([0,1],[0,1], linestyle=\"--\")\n\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve \u2014 Baseline vs New\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()"
        },
        {
          "type": "image",
          "src": "/images/2.0/cell_17_img_01.png",
          "alt": "cell_17_img_01.png"
        },
        {
          "type": "stdout",
          "content": "<Figure size 600x500 with 1 Axes>"
        },
        {
          "type": "code",
          "language": "python",
          "content": "plt.figure(figsize=(6,5))\n\n# New (raw)\nprec_n, rec_n, _ = precision_recall_curve(y_test, p_test)\nap_n = average_precision_score(y_test, p_test)\nplt.plot(rec_n, prec_n, label=f\"New (raw) AP={ap_n:.3f}\")\n\n# New (cal)\nprec_c, rec_c, _ = precision_recall_curve(y_test, p_test_cal)\nap_c = average_precision_score(y_test, p_test_cal)\nplt.plot(rec_c, prec_c, label=f\"New (cal) AP={ap_c:.3f}\")\n\n# Baseline (if available)\nif \"p_test_base\" in globals():\n    prec_b, rec_b, _ = precision_recall_curve(y_test, p_test_base)\n    ap_b = average_precision_score(y_test, p_test_base)\n    plt.plot(rec_b, prec_b, label=f\"Baseline AP={ap_b:.3f}\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precision\u2013Recall \u2014 Baseline vs New\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()"
        },
        {
          "type": "image",
          "src": "/images/2.0/cell_18_img_02.png",
          "alt": "cell_18_img_02.png"
        },
        {
          "type": "stdout",
          "content": "<Figure size 600x500 with 1 Axes>"
        },
        {
          "type": "code",
          "language": "python",
          "content": "def plot_cal_curve(y, p, label):\n    frac_pos, mean_pred = calibration_curve(y, p, n_bins=12, strategy=\"uniform\")\n    plt.plot(mean_pred, frac_pos, marker=\"o\", label=label)\n\nplt.figure(figsize=(6,5))\nplot_cal_curve(y_test, p_test, \"New (raw)\")\nplot_cal_curve(y_test, p_test_cal, \"New (cal)\")\nif \"p_test_base\" in globals():\n    plot_cal_curve(y_test, p_test_base, \"Baseline\")\n\n# Perfect calibration line\nxs = np.linspace(0,1,100)\nplt.plot(xs, xs, linestyle=\"--\", label=\"Perfect\")\n\nplt.xlabel(\"Mean predicted probability\")\nplt.ylabel(\"Observed frequency\")\nplt.title(\"Reliability Diagram\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()"
        },
        {
          "type": "image",
          "src": "/images/2.0/cell_19_img_03.png",
          "alt": "cell_19_img_03.png"
        },
        {
          "type": "stdout",
          "content": "<Figure size 600x500 with 1 Axes>"
        },
        {
          "type": "code",
          "language": "python",
          "content": "def sweep_metrics(y, p, label):\n    thrs = np.linspace(0.1, 0.9, 33)\n    accs = []\n    f1s  = []\n    for t in thrs:\n        yhat = (p >= t).astype(int)\n        accs.append(accuracy_score(y, yhat))\n        f1s.append(f1_score(y, yhat))\n    return thrs, np.array(accs), np.array(f1s), label\n\n# Accuracy\nplt.figure(figsize=(6,5))\nth, acc_new, f1_new, lab_n = sweep_metrics(y_test, p_test, \"New (raw)\")\nplt.plot(th, acc_new, label=\"New (raw)\")\nth, acc_cal, f1_cal, lab_c = sweep_metrics(y_test, p_test_cal, \"New (cal)\")\nplt.plot(th, acc_cal, label=\"New (cal)\")\nif \"p_test_base\" in globals():\n    th, acc_b, f1_b, lab_b = sweep_metrics(y_test, p_test_base, \"Baseline\")\n    plt.plot(th, acc_b, label=\"Baseline\")\n\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Threshold Sweep \u2014 Accuracy\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\n\n# F1\nplt.figure(figsize=(6,5))\nplt.plot(th, f1_new, label=\"New (raw)\")\nplt.plot(th, f1_cal, label=\"New (cal)\")\nif \"p_test_base\" in globals():\n    plt.plot(th, f1_b, label=\"Baseline\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"F1\")\nplt.title(\"Threshold Sweep \u2014 F1\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()"
        },
        {
          "type": "image",
          "src": "/images/2.0/cell_20_img_04.png",
          "alt": "cell_20_img_04.png"
        },
        {
          "type": "stdout",
          "content": "<Figure size 600x500 with 1 Axes>"
        },
        {
          "type": "image",
          "src": "/images/2.0/cell_20_img_05.png",
          "alt": "cell_20_img_05.png"
        },
        {
          "type": "stdout",
          "content": "<Figure size 600x500 with 1 Axes>"
        },
        {
          "type": "code",
          "language": "python",
          "content": "thr = 0.55  # you used ~0.55 earlier; change if needed\n\ndef plot_cm(y, p, title):\n    yhat = (p >= thr).astype(int)\n    cm = confusion_matrix(y, yhat)\n    cmn = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n\n    plt.figure(figsize=(5,4))\n    plt.imshow(cmn, interpolation=\"nearest\")\n    plt.title(title)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    for (i, j), v in np.ndenumerate(cmn):\n        plt.text(j, i, f\"{v:.2f}\", ha=\"center\", va=\"center\")\n    plt.xticks([0,1],[0,1])\n    plt.yticks([0,1],[0,1])\n    plt.colorbar()\n    plt.tight_layout()\n\nplot_cm(y_test, p_test, \"New (raw) \u2014 Confusion Matrix (normalized)\")\nplot_cm(y_test, p_test_cal, \"New (cal) \u2014 Confusion Matrix (normalized)\")\nif \"p_test_base\" in globals():\n    plot_cm(y_test, p_test_base, \"Baseline \u2014 Confusion Matrix (normalized)\")"
        },
        {
          "type": "image",
          "src": "/images/2.0/cell_21_img_06.png",
          "alt": "cell_21_img_06.png"
        },
        {
          "type": "stdout",
          "content": "<Figure size 500x400 with 2 Axes>"
        },
        {
          "type": "image",
          "src": "/images/2.0/cell_21_img_07.png",
          "alt": "cell_21_img_07.png"
        },
        {
          "type": "stdout",
          "content": "<Figure size 500x400 with 2 Axes>"
        },
        {
          "type": "code",
          "language": "python",
          "content": "def cumulative_gain(y_true, p_scores, label):\n    # sort by predicted prob desc\n    order = np.argsort(-p_scores)\n    y_sorted = np.array(y_true)[order]\n    cum_pos = np.cumsum(y_sorted)\n    total_pos = y_sorted.sum()\n    frac_samples = np.arange(1, len(y_sorted)+1) / len(y_sorted)\n    gain = cum_pos / max(total_pos, 1)\n    return frac_samples, gain, label\n\nplt.figure(figsize=(6,5))\nx, g_new, _ = cumulative_gain(y_test, p_test, \"New (raw)\")\nplt.plot(x, g_new, label=\"New (raw)\")\nx, g_cal, _ = cumulative_gain(y_test, p_test_cal, \"New (cal)\")\nplt.plot(x, g_cal, label=\"New (cal)\")\n\nif \"p_test_base\" in globals():\n    x, g_base, _ = cumulative_gain(y_test, p_test_base, \"Baseline\")\n    plt.plot(x, g_base, label=\"Baseline\")\n\n# Random model line\nplt.plot([0,1],[0,1], linestyle=\"--\", label=\"Random\")\n\nplt.xlabel(\"Fraction of plays reviewed (highest p first)\")\nplt.ylabel(\"Fraction of actual passes captured\")\nplt.title(\"Cumulative Gain (Focus on identifying passes)\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()"
        },
        {
          "type": "image",
          "src": "/images/2.0/cell_22_img_08.png",
          "alt": "cell_22_img_08.png"
        },
        {
          "type": "stdout",
          "content": "<Figure size 600x500 with 1 Axes>"
        },
        {
          "type": "code",
          "language": "python",
          "content": "booster = final_clf.booster_\nimp = pd.DataFrame({\n    \"feature\": booster.feature_name(),\n    \"gain\": booster.feature_importance(importance_type=\"gain\"),\n    \"split\": booster.feature_importance(importance_type=\"split\"),\n})\nimp = imp.sort_values(\"gain\", ascending=False).head(20)\n\nplt.figure(figsize=(7,6))\nplt.barh(imp[\"feature\"][::-1], imp[\"gain\"][::-1])\nplt.title(\"Top-20 Features by Gain (Final Model)\")\nplt.xlabel(\"Gain\")\nplt.tight_layout()"
        },
        {
          "type": "image",
          "src": "/images/2.0/cell_23_img_09.png",
          "alt": "cell_23_img_09.png"
        },
        {
          "type": "stdout",
          "content": "<Figure size 700x600 with 1 Axes>"
        },
        {
          "type": "code",
          "language": "python",
          "content": "# Build eval frame\neval_df = pd.DataFrame({\"y\": y_test.values, \"p\": p_test})\nif \"p_test_cal\" in globals():   eval_df[\"p_cal\"]  = p_test_cal\nif \"p_test_base\" in globals():  eval_df[\"p_base\"] = p_test_base\n\n# Pull situation features from X_test_raw\nyds = (X_test_raw[\"next_ydstogo\"]\n       if \"next_ydstogo\" in X_test_raw.columns\n       else X_test_raw.filter(like=\"ydstogo\").iloc[:, 0])\ndown = (X_test_raw[\"next_down\"]\n        if \"next_down\" in X_test_raw.columns\n        else X_test_raw.filter(regex=r\"(^|_)down(_|$)\").iloc[:, 0])\n\neval_df[\"down\"] = pd.to_numeric(down, errors=\"coerce\")\neval_df[\"dist\"] = pd.cut(\n    yds, bins=[-np.inf, 3, 6, 10, np.inf],\n    labels=[\"<=3\", \"4\u20136\", \"7\u201310\", \">10\"]\n).astype(str)\n\ndef group_metrics(df, prob_col):\n    rows = []\n    for (d, dist), g in df.groupby([\"down\", \"dist\"], dropna=True):\n        y, p = g[\"y\"].values, g[prob_col].values\n        auc = roc_auc_score(y, p) if len(np.unique(y)) > 1 else np.nan\n        gap = float(abs(p.mean() - y.mean()))  # mean pred vs empirical rate\n        rows.append({\"down\": d, \"dist\": dist, \"n\": len(g), \"auc\": auc, \"abs_gap\": gap})\n    m = pd.DataFrame(rows)\n    m = m.sort_values([\"down\", \"dist\"])\n    return m\n\nmodels = {\"New (raw)\": \"p\"}\nif \"p_test_cal\"  in eval_df.columns: models[\"New (cal)\"]  = \"p_cal\"\nif \"p_test_base\" in eval_df.columns: models[\"Baseline\"]   = \"p_base\"\n\nfor name, col in models.items():\n    M = group_metrics(eval_df, col)\n    auc_p  = M.pivot(index=\"down\", columns=\"dist\", values=\"auc\")\n    gap_p  = M.pivot(index=\"down\", columns=\"dist\", values=\"abs_gap\")\n\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n    # AUC heatmap\n    im0 = axes[0].imshow(auc_p, vmin=0.50, vmax=0.90)\n    axes[0].set_title(f\"{name} \u2014 AUC by down \u00d7 distance\")\n    axes[0].set_xticks(range(len(auc_p.columns))); axes[0].set_xticklabels(auc_p.columns)\n    axes[0].set_yticks(range(len(auc_p.index)));   axes[0].set_yticklabels(auc_p.index.astype(int))\n    for i in range(auc_p.shape[0]):\n        for j in range(auc_p.shape[1]):\n            val = auc_p.iloc[i, j]\n            if pd.notnull(val):\n                axes[0].text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\", fontsize=9)\n    fig.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04)\n\n    # Calibration gap heatmap\n    im1 = axes[1].imshow(gap_p, vmin=0.00, vmax=0.15)\n    axes[1].set_title(f\"{name} \u2014 |Calibration gap| by down \u00d7 distance\")\n    axes[1].set_xticks(range(len(gap_p.columns))); axes[1].set_xticklabels(gap_p.columns)\n    axes[1].set_yticks(range(len(gap_p.index)));   axes[1].set_yticklabels(gap_p.index.astype(int))\n    for i in range(gap_p.shape[0]):\n        for j in range(gap_p.shape[1]):\n            val = gap_p.iloc[i, j]\n            if pd.notnull(val):\n                axes[1].text(j, i, f\"{val:.02f}\", ha=\"center\", va=\"center\", fontsize=9)\n    fig.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)\n\n    plt.tight_layout()"
        },
        {
          "type": "image",
          "src": "/images/2.0/cell_24_img_10.png",
          "alt": "cell_24_img_10.png"
        },
        {
          "type": "stdout",
          "content": "<Figure size 1000x400 with 4 Axes>"
        }
      ]
    }
  ]
}